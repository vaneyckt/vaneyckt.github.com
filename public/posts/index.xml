<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on vaneyckt</title>
    <link>http://vaneyckt.io/posts/</link>
    <description>Recent content in Posts on vaneyckt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Tom Van Eyck</copyright>
    <lastBuildDate>Sun, 16 Aug 2015 19:43:34 +0000</lastBuildDate>
    <atom:link href="http://vaneyckt.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Installing Chromedriver</title>
      <link>http://vaneyckt.io/posts/installing_chromedriver/</link>
      <pubDate>Sun, 16 Aug 2015 19:43:34 +0000</pubDate>
      
      <guid>http://vaneyckt.io/posts/installing_chromedriver/</guid>
      <description>&lt;p&gt;Some time ago I needed to install &lt;a href=&#34;https://sites.google.com/a/chromium.org/chromedriver/&#34;&gt;chromedriver&lt;/a&gt; on a ubuntu machine. While this wasn&amp;rsquo;t too hard, I was nevertheless surprised by the number of open &lt;a href=&#34;https://stackoverflow.com/&#34;&gt;StackOverflow&lt;/a&gt; questions on this topic. So I decided to leave some notes for my future self.&lt;/p&gt;

&lt;p&gt;First of all, let&amp;rsquo;s install chromedriver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ LATEST_RELEASE=$(curl http://chromedriver.storage.googleapis.com/LATEST_RELEASE)
$ wget http://chromedriver.storage.googleapis.com/$LATEST_RELEASE/chromedriver_linux64.zip
$ unzip chromedriver_linux64.zip
$ rm chromedriver_linux64.zip
$ sudo mv chromedriver /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what happens when we try and run it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chromedriver
    chromedriver: error while loading shared libraries: libgconf-2.so.4:
    cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a bit unexpected. Luckily we can easily fix this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install libgconf-2-4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have a functioning chromedriver, the only thing left to do is to install Chrome. After all, chromedriver can&amp;rsquo;t function without Chrome.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
$ sudo sh -c &#39;echo &amp;quot;deb http://dl.google.com/linux/chrome/deb/ stable main&amp;quot; &amp;gt;&amp;gt; /etc/apt/sources.list.d/google.list&#39;
$ sudo apt-get update
$ sudo apt-get install google-chrome-stable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. You should be good to go now.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cheap MapReduce in Go</title>
      <link>http://vaneyckt.io/posts/cheap-mapreduce-in-go/</link>
      <pubDate>Mon, 13 Jul 2015 11:07:43 -0400</pubDate>
      
      <guid>http://vaneyckt.io/posts/cheap-mapreduce-in-go/</guid>
      <description>

&lt;p&gt;Here at &lt;a href=&#34;http://www.malwarebytes.org&#34;&gt;Malwarebytes&lt;/a&gt; we have to deal with a huge amount of data generated by many different systems, malware research teams, telemetry, etc. That data can span for several Terabytes on a daily basis.&lt;/p&gt;

&lt;p&gt;We have been using Amazon&amp;rsquo;s Elastic MapReduce (EMR) in many different occasions, but we are always looking to simplify our systems wherever we can. Before I start another flame-war about this topic and why we decided to create a MapReduce job that runs in a single machine as oppose to a cluster of EC2 instances, let me explain. When creating complex systems that leverages a cluster of machines, it comes with a lot of extra costs from DevOps, IT, deployments and many other things that now we need to mantain and keep up-to-date. There are cases where we would like to churn over tons of data, but we don&amp;rsquo;t need to be super fast, or leverage a huge complex infrastructure, therefore reducing lots of the costs and maintenance associated with it.&lt;/p&gt;

&lt;p&gt;Other larger MapReduce jobs still runs within EMR, since we need more scalability and faster turn around times, and wouldn&amp;rsquo;t be appropriate to be executed in a single machine.&lt;/p&gt;

&lt;h3 id=&#34;the-goal:1581d86ae1166838a7c11f8a2972c300&#34;&gt;The Goal&lt;/h3&gt;

&lt;p&gt;We have a bunch of different anonymous telemetry systems that collects tons of data every second, and some of our executives wanted to have some data aggregated into CSV files that could be loaded into Excel and a couple of other analytics systems to generate custom Pivot Tables, so we can better understand some of the usage patterns and dig information from our telemetry, without us needing to write complicated reporting systems that would like change a lot.&lt;/p&gt;

&lt;p&gt;Below is a stripped down version of our telemetry data so you guys can better understand what we were trying to accomplish.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Request&amp;quot;: {
        &amp;quot;time&amp;quot;: &amp;quot;2015-07-13 18:37:00&amp;quot;,
        &amp;quot;processedTime&amp;quot;: &amp;quot;2015-07-13 18:37:14&amp;quot;,
        &amp;quot;uuid&amp;quot;: &amp;quot;56ca2dbffc5f451285bade8e8ffef12c&amp;quot;,
        &amp;quot;jobId&amp;quot;: &amp;quot;ffffffc8f9af58389681e4a9749a4e6c&amp;quot;,
        &amp;quot;Sender&amp;quot;: &amp;quot;scanner&amp;quot;,
        &amp;quot;Trigger&amp;quot;: &amp;quot;update&amp;quot;
    },
    &amp;quot;App&amp;quot;: {
        &amp;quot;Program&amp;quot;: &amp;quot;app&amp;quot;,
        &amp;quot;Build&amp;quot;: &amp;quot;consumer&amp;quot;,
        &amp;quot;License&amp;quot;: &amp;quot;licensed&amp;quot;,
        &amp;quot;Version&amp;quot;: &amp;quot;2.1.8&amp;quot;
    },
    &amp;quot;Connection&amp;quot;: {
        &amp;quot;Type&amp;quot;: &amp;quot;broadband&amp;quot;,
        &amp;quot;ISP&amp;quot;: &amp;quot;Telecom Italia&amp;quot;
    },
    &amp;quot;Region&amp;quot;: {
        &amp;quot;Continent&amp;quot;: &amp;quot;EU&amp;quot;,
        &amp;quot;Country&amp;quot;: &amp;quot;IT&amp;quot;,
        &amp;quot;Region&amp;quot;: &amp;quot;08&amp;quot;,
        &amp;quot;City&amp;quot;: &amp;quot;Genoa&amp;quot;
    },
    &amp;quot;Client&amp;quot;: {
        &amp;quot;OsVersion&amp;quot;: &amp;quot;Windows 7 Service Pack 1&amp;quot;,
        &amp;quot;Language&amp;quot;: &amp;quot;it&amp;quot;,
        &amp;quot;Architecture&amp;quot;: &amp;quot;x64&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, we have some simple telemetry data that our collector machines generate constantly from a few different systems. These JSON records gets aggregated and saved into thousands of different files into batches that can contain hundreds of thousands of lines each.&lt;/p&gt;

&lt;p&gt;Each file contains some header comments with information from the collector and some metadata about this. Here is an example of one of these files that we need to process.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  Type:				collector
#  Queue:				client
#  Processed At:		2015-07-13 18:37:14
#  Format:				json
#  Program Version:		2015-04-15
#  EC2 Instance Id:		i-d561a6fa
{ &amp;quot;Request&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;2015-07-13 18:37:00&amp;quot;, &amp;quot;processedTime&amp;quot;: &amp;quot;2015-07-13 18:37:14&amp;quot;, ...
{ &amp;quot;Request&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;2015-07-13 18:37:00&amp;quot;, &amp;quot;processedTime&amp;quot;: &amp;quot;2015-07-13 18:37:14&amp;quot;, ...
{ &amp;quot;Request&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;2015-07-13 18:37:00&amp;quot;, &amp;quot;processedTime&amp;quot;: &amp;quot;2015-07-13 18:37:14&amp;quot;, ...
{ &amp;quot;Request&amp;quot;: { &amp;quot;time&amp;quot;: &amp;quot;2015-07-13 18:37:00&amp;quot;, &amp;quot;processedTime&amp;quot;: &amp;quot;2015-07-13 18:37:14&amp;quot;, ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this particular job requirement, we needed to aggregate counts of the unique combination that some of these fields have, in order to generate proper data for our Pivot Tables. Therefore this was a job for a MapReduce implementation, where we would aggregate the unique occurrences of each permutation of the data and then reduce the counts into the desired aggregation.&lt;/p&gt;

&lt;p&gt;We wanted to ignore some fields in the JSON record, so we come up with a &lt;code&gt;Telemetry&lt;/code&gt; structure that would map the fields that we would like to uniquely aggregate from. Here is what we came up with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Telemetry struct {
	Request struct {
    	Sender  string `json:&amp;quot;Sender,omitempty&amp;quot;`
    	Trigger string `json:&amp;quot;Trigger,omitempty&amp;quot;`
    } `json:&amp;quot;Request,omitempty&amp;quot;`

	App struct {
    	Program  string `json:&amp;quot;Program,omitempty&amp;quot;`
    	Build    string `json:&amp;quot;Build,omitempty&amp;quot;`
    	License  string `json:&amp;quot;License,omitempty&amp;quot;`
    	Version  string `json:&amp;quot;Version,omitempty&amp;quot;`
    } `json:&amp;quot;App,omitempty&amp;quot;`

	Connection struct {
    	Type string `json:&amp;quot;Type,omitempty&amp;quot;`
    } `json:&amp;quot;Connection,omitempty&amp;quot;`

	Region struct {
    	Continent string `json:&amp;quot;Continent,omitempty&amp;quot;`
    	Country   string `json:&amp;quot;Country,omitempty&amp;quot;`
    } `json:&amp;quot;Region,omitempty&amp;quot;`

	Client struct {
    	OsVersion    string `json:&amp;quot;OsVersion,omitempty&amp;quot;`
    	Language     string `json:&amp;quot;Language,omitempty&amp;quot;`
    	Architecture string `json:&amp;quot;Architecture,omitempty&amp;quot;`
    } `json:&amp;quot;Client,omitempty&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So any permutation occurrence of any of these fields above, would generate a single line, with the correspondent occurrence count in the final output of our MapReduce.&lt;/p&gt;

&lt;p&gt;Now that we have a better understanding on what were trying to accomplish, let&amp;rsquo;s look how we did implement the MapReduce for this.&lt;/p&gt;

&lt;h3 id=&#34;enumerating-files-for-the-job:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Enumerating Files for the Job&lt;/h3&gt;

&lt;p&gt;In order to find files that we need to feed into our MapReduce engine, we would need to create a method to enumerate all the files in directory, and potentially sub-directories. For that, we will leverage the &lt;code&gt;Walk()&lt;/code&gt; method inside the &lt;code&gt;filepath&lt;/code&gt; package that is part of the standard library.&lt;/p&gt;

&lt;p&gt;This method takes a walk function that has the following method signature:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type WalkFunc func(path string, info os.FileInfo, err error) error
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have implemented the enumeration function like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func enumerateFiles(dirname string) chan interface{} {
	output := make(chan interface{})
	go func() {
		filepath.Walk(dirname, func(path string, f os.FileInfo, err error) error {
			if !f.IsDir() {
				output &amp;lt;- path
			}
			return nil
		})
		close(output)
	}()
	return output
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function will first create a channel that will receive each file that is encountered by the Walk function, which will be used later in the our mapper function.&lt;/p&gt;

&lt;h3 id=&#34;generating-tasks-for-our-mapper:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Generating Tasks for our Mapper&lt;/h3&gt;

&lt;p&gt;Another method that we need to write is one that will parse a single task file and enumerate all JSON records. Remember, we saved each JSON record as a separate individual line in our task file. We also need to account for the meta headers and ignore those.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func enumerateJSON(filename string) chan string {
	output := make(chan string)
	go func() {
		file, err := os.Open(filename)
		if err != nil {
			return
		}
		defer file.Close()
		reader := bufio.NewReader(file)
		for {
			line, err := reader.ReadString(&#39;\n&#39;)
			if err == io.EOF {
				break
			}

			// ignore any meta comments on top of JSON file
			if strings.HasPrefix(line, &amp;quot;#&amp;quot;) == true {
				continue
			}

			// add each json line to our enumeration channel
			output &amp;lt;- line
		}
		close(output)
	}()
	return output
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;defining-our-interface:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Defining our interface&lt;/h3&gt;

&lt;p&gt;For our MapReduce implementation we would need to define our collector type and a few function types that we will use later in the process. Here is what we came up with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// MapperCollector is a channel that collects the output from mapper tasks
type MapperCollector chan chan interface{}

// MapperFunc is a function that performs the mapping part of the MapReduce job
type MapperFunc func(interface{}, chan interface{})

// ReducerFunc is a function that performs the reduce part of the MapReduce job
type ReducerFunc func(chan interface{}, chan interface{})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, we are leverage Go channels a lot here, and this will be the key to bridge all this together.&lt;/p&gt;

&lt;h3 id=&#34;the-mapper:1581d86ae1166838a7c11f8a2972c300&#34;&gt;The Mapper&lt;/h3&gt;

&lt;p&gt;Onto the mapper function. The whole idea on this mapper implementation is to parse a single file and go over each JSON record that were enumerated, decoding the JSON content into our Telemetry structure and accumulate counts for each dimension (unique permutation of the data).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func mapper(filename interface{}, output chan interface{}) {
	results := map[Telemetry]int{}

    // start the enumeration of each JSON lines in the file
	for line := range enumerateJSON(filename.(string)) {

		// decode the telemetry JSON line
		dec := json.NewDecoder(strings.NewReader(line))
		var telemetry Telemetry

		// if line cannot be JSON decoded then skip to next one
		if err := dec.Decode(&amp;amp;telemetry); err == io.EOF {
			continue
		} else if err != nil {
			continue
		}

		// stores Telemetry structure in the mapper results dictionary
		previousCount, exists := results[telemetry]
		if !exists {
			results[telemetry] = 1
		} else {
			results[telemetry] = previousCount + 1
		}
	}

	output &amp;lt;- results
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The trick on this mapper function lies into the way we define our Map to accumulate unique data. We defined a map in which the Key in our Telemetry structure as below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;results := map[Telemetry]int{}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remember, in Go map keys may be of any type that is comparable. The language spec defines this precisely, but in short, comparable types are boolean, numeric, string, pointer, channel, and interface types, and structs or arrays that contain only those types. Notably absent from the list are slices, maps, and functions; these types cannot be compared using ==, and may not be used as map keys. It&amp;rsquo;s obvious that strings, ints, and other basic types should be available as map keys, but perhaps unexpected are struct keys. Struct can be used to key data by multiple dimensions.&lt;/p&gt;

&lt;h3 id=&#34;the-reducer:1581d86ae1166838a7c11f8a2972c300&#34;&gt;The Reducer&lt;/h3&gt;

&lt;p&gt;Now for the reducer part of our job, we would simply need to aggregate the different Telemetry dimensions that were generated by all the different mappers that were ran in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func reducer(input chan interface{}, output chan interface{}) {
	results := map[Telemetry]int{}
	for matches := range input {
		for key, value := range matches.(map[Telemetry]int) {
			_, exists := results[key]
			if !exists {
				results[key] = value
			} else {
				results[key] = results[key] + value
			}
		}
	}
	output &amp;lt;- results
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;dispatching-tasks:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Dispatching Tasks&lt;/h2&gt;

&lt;p&gt;In this MapReduce implementation, were are leveraging channels for the different inputs and outputs from file enumeration, to Mappers and finally Reducers. We need to create some dispatcher functions to bridge all this together in invoke the next step in each case.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func mapperDispatcher(mapper MapperFunc, input chan interface{}, collector MapperCollector) {
	for item := range input {
		taskOutput := make(chan interface{})
		go mapper(item, taskOutput)
		collector &amp;lt;- taskOutput
	}
	close(collector)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;mapperDispatcher&lt;/code&gt; function is responsible to listen on the input channel that receives each filename to be processed and invoke a &lt;code&gt;mapper&lt;/code&gt; for each file, pushing the output of the job into a &lt;code&gt;MapperCollector&lt;/code&gt;, that would be used in the next step.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func reducerDispatcher(collector MapperCollector, reducerInput chan interface{}) {
	for output := range collector {
		reducerInput &amp;lt;- &amp;lt;-output
	}
	close(reducerInput)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;reducerDispatcher&lt;/code&gt; function is responsible to listen on the collector channel and push each item as the input for the Reducer task.&lt;/p&gt;

&lt;h3 id=&#34;putting-all-together-in-a-mapreduce-method:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Putting all together in a MapReduce method&lt;/h3&gt;

&lt;p&gt;Now that we have all the pieces of the puzzle, it is time for us to put all together into a MapReduce function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;const (
	MaxWorkers = 10
)

func mapReduce(mapper MapperFunc, reducer ReducerFunc, input chan interface{}) interface{} {

	reducerInput := make(chan interface{})
	reducerOutput := make(chan interface{})
	mapperCollector := make(MapperCollector, MaxWorkers)

	go reducer(reducerInput, reducerOutput)
	go reducerDispatcher(mapperCollector, reducerInput)
	go mapperDispatcher(mapper, input, mapperCollector)

	return &amp;lt;-reducerOutput
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can observe, we are creating all the required channels that would be the conduit and bridge of this entire operation.&lt;/p&gt;

&lt;p&gt;First, we spawn a go routine that will be responsible for executing the Reducer task, listening for the input channels to operate on the data and outputing it&amp;rsquo;s task into the output channel when everything is done. Secondly, in order for the whole system to work, we need to start the dispatcher go routines that will bridge all this together invoking the next steps. The &lt;code&gt;mapperDispatcher&lt;/code&gt; is responsible to invoke the &lt;code&gt;mapper&lt;/code&gt; function that will trigger the whole MapReduce calculation.&lt;/p&gt;

&lt;p&gt;We are limiting the number of concurrent mappers to 10 in this case, but we could control the amount of concurrency of over how many mappers are simultaneously opening the tasks files and aggregating data.&lt;/p&gt;

&lt;p&gt;Finally, we have written our &lt;code&gt;main()&lt;/code&gt; function like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import (
	&amp;quot;bufio&amp;quot;
	&amp;quot;encoding/csv&amp;quot;
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;io&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;path/filepath&amp;quot;
	&amp;quot;runtime&amp;quot;
	&amp;quot;strconv&amp;quot;
	&amp;quot;strings&amp;quot;
)

...

func main() {
	runtime.GOMAXPROCS(runtime.NumCPU())
	fmt.Println(&amp;quot;Processing. Please wait....&amp;quot;)

    // start the enumeration of files to be processed into a channel
	input := enumerateFiles(&amp;quot;.&amp;quot;)

    // this will start the map reduce work
	results := mapReduce(mapper, reducer, input)

	// open output file
	f, err := os.Create(&amp;quot;telemetry.csv&amp;quot;)
	if err != nil {
		panic(err)
	}
	defer f.Close()

	// make a write buffer
	writer := csv.NewWriter(f)

	for telemetry, value := range results.(map[Telemetry]int) {

		var record []string

		record = append(record, telemetry.Request.Sender)
		record = append(record, telemetry.Request.Trigger)
		record = append(record, telemetry.App.Program)
		record = append(record, telemetry.App.Build)
		record = append(record, telemetry.App.License)
		record = append(record, telemetry.App.Version)
		record = append(record, telemetry.Connection.Type)
		record = append(record, telemetry.Region.Continent)
		record = append(record, telemetry.Region.Country)
		record = append(record, telemetry.Client.OsVersion)
		record = append(record, telemetry.Client.Language)
		record = append(record, telemetry.Client.Architecture)

        // The last field of the CSV line is the aggregate count for each occurrence
		record = append(record, strconv.Itoa(value))

		writer.Write(record)
	}

	writer.Flush()

	fmt.Println(&amp;quot;Done!&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step is to kick off the &lt;code&gt;enumerateFiles(...)&lt;/code&gt; function to start enumerating files to be processed that will be pushed into a input channel. Then we invoke the &lt;code&gt;mapReduce(...)&lt;/code&gt; method that will perform the entire Job returning an array of the end results.&lt;/p&gt;

&lt;p&gt;As a final step, we write the entire MapReduce results into a CSV file, printing each Telemetry dimension and it&amp;rsquo;s respective aggregate count in each line.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:1581d86ae1166838a7c11f8a2972c300&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Sometimes you don&amp;rsquo;t need overly complex infrastructures or systems to do a job well. In this case, we were running these exact same aggregations over close to 20 EMR instances that would take a few minutes to execute the entire MapReduce job over hundreds of Gigabytes of data each day.&lt;/p&gt;

&lt;p&gt;When we decided to take a look at this problem again, we rewrote this task using Go, and we now simply run this on a single 8-core machine and the whole daily execution takes about 10 minutes. We cut a lot of the costs associated with maintaining and running these EMR systems and we just schedule this Go app to run once a day over our daily dataset.&lt;/p&gt;

&lt;p&gt;You can find the entire code here:&lt;br /&gt;
&lt;a href=&#34;https://gist.github.com/mcastilho/e051898d129b44e2f502&#34;&gt;https://gist.github.com/mcastilho/e051898d129b44e2f502&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Singleton Pattern in Go</title>
      <link>http://vaneyckt.io/posts/singleton-pattern-in-go/</link>
      <pubDate>Sun, 12 Jul 2015 13:50:25 -0400</pubDate>
      
      <guid>http://vaneyckt.io/posts/singleton-pattern-in-go/</guid>
      <description>

&lt;p&gt;The Go Language growth has been phenomenal in the last few years, and is attracting language converts from all walks of life. There has been a lot articles recently about companies switching from Ruby, and experiencing the new world of Go and it&amp;rsquo;s parallel and concurrent approach to problems.&lt;/p&gt;

&lt;p&gt;In the last 10 years, Ruby on Rails has allowed many developers and startups to quickly develop powerful systems, most of the time without needing to worry on how the inner things work, or worry about thread-safety and concurrency. It is very rare for a RoR application to create threads and run things in parallel. The whole hosting infrastructure and framework stack uses a different approach, by parallelizing it via multiple processes. It has only been in the last few years that multithreaded rack servers like &lt;a href=&#34;http://puma.io/&#34;&gt;Puma&lt;/a&gt; has surged in popularity, but even that brought a lot of issues in the beggining with third-party gems and other code that weren&amp;rsquo;t designed to be thread safe.&lt;/p&gt;

&lt;p&gt;Now with a lot of new developers embarking into the Go Language boat, we need to carefully look at our code and see how it will behave, it needs to be designed in a  thread-safe way.&lt;/p&gt;

&lt;h3 id=&#34;the-common-mistake:dad286903448e7ad7641ef7a6de34a28&#34;&gt;The Common Mistake&lt;/h3&gt;

&lt;p&gt;Recently, I have seen this kind of mistake more and more in github repositories. Singleton implementations that doesn&amp;rsquo;t have any consideration for thread-safety. Below is the most common example of this mistake.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package singleton

type singleton struct {
}

var instance *singleton

func GetInstance() *singleton {
	if instance == nil {
		instance = &amp;amp;singleton{}   // &amp;lt;--- NOT THREAD SAFE
	}
	return instance
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the above scenario, multiple go routines could evaluate the first check and they would all create an instance of the &lt;code&gt;singleton&lt;/code&gt; type and override each other. There is no guarantee which instance it will be returned here, and other further operations on the instance can be come inconsistent with the expectations by the developer.&lt;/p&gt;

&lt;p&gt;The reason this is bad is that if references to the singleton instance are being held around through the code, there could be potentially multiple instances of the type with different states, generating potential different code behaviours. It also becomes a real nightmare during debugging and becomes really hard to spot the the bug, since that at debugging time nothing really appears to be wrong due to the run-time pauses minimizing the potential of a non-thread-safe execution, easily hiding the problem from the developer.&lt;/p&gt;

&lt;h3 id=&#34;the-aggressive-locking:dad286903448e7ad7641ef7a6de34a28&#34;&gt;The Aggressive Locking&lt;/h3&gt;

&lt;p&gt;I have also seen this poor solution to the thread-safety problem. Indeed this solves the thread-safety issue, but creates other potential serious problems. It introduces a threading contention by perform aggressive locking of the entire method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var mu Sync.Mutex

func GetInstance() *singleton {
    mu.Lock()                    // &amp;lt;--- Unnecessary locking if instance already created
    defer mu.Unlock()

    if instance == nil {
        instance = &amp;amp;singleton{}
    }
    return instance
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the code above, we can see that we solve the thread-safety issue by introducing the &lt;code&gt;Sync.Mutex&lt;/code&gt; and acquiring the Lock before creating the singleton instance. The problem is that here we are performing excessive locking even when we wouldn&amp;rsquo;t be required to do so, in the case the instance has been already created and we should simply have returned the cached singleton instance. On a highly concurrent code base, this can generate a bottle-neck since only one go routine could get the singleton instance at a time.&lt;/p&gt;

&lt;p&gt;So, this is not the best approach. We have to look at other solutions.&lt;/p&gt;

&lt;h3 id=&#34;check-lock-check-pattern:dad286903448e7ad7641ef7a6de34a28&#34;&gt;Check-Lock-Check Pattern&lt;/h3&gt;

&lt;p&gt;In C++ and other languages, the best and safest way to ensure minimal locking and still be thread-safe is to utilize the well known pattern called Check-Lock-Check, when acquiring locks. The pseudo-code for the pattern is something like this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if check() {
    lock() {
        if check() {
            // perform your lock-safe code here
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The idea behind this pattern is that you would want to check first, to minimize any aggressive locking, since an IF statement is less expensive than the locking. Secondly we would want to wait and acquire the exclusive lock so only one execution is inside that block at a single time. But butween the first check and the acquisition of the exclusive lock there could have been another thread that did acquire the lock, therefore we would need to check again inside the lock to avoid replacing the instance with another one.&lt;/p&gt;

&lt;p&gt;Over the years, the people that has worked with me knows this well, that I have been very strict with my engineering teams during code-reviews about this pattern and thread-safety mentality.&lt;/p&gt;

&lt;p&gt;If we apply this pattern to our &lt;code&gt;GetInstance()&lt;/code&gt; method we would have something as follow:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func GetInstance() *singleton {
    if instance == nil {     // &amp;lt;-- Not yet perfect. since it&#39;s not fully atomic
        mu.Lock()
        defer mu.Unlock()

        if instance == nil {
            instance = &amp;amp;singleton{}
        }
    }
    return instance
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a better approach, but still is &lt;strong&gt;not&lt;/strong&gt; perfect. Since due to compiler optimizations there is not an atomic check on the instance store state. With all the technical considerations this is still not perfect. But it is much better than the initial approach.&lt;/p&gt;

&lt;p&gt;But using the &lt;code&gt;sync/atomic&lt;/code&gt; package, we can atomically load and set a flag that will indicate if we have initialized or not our instance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;import &amp;quot;sync&amp;quot;
import &amp;quot;sync/atomic&amp;quot;

var initialized uint32
...

func GetInstance() *singleton {

    if atomic.LoadUInt32(&amp;amp;initialized) == 1 {
		return instance
	}

    mu.Lock()
    defer mu.Unlock()

    if initialized == 0 {
         instance = &amp;amp;singleton{}
         atomic.StoreUint32(&amp;amp;initialized, 1)
    }

    return instance
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But&amp;hellip; I believe we could do better by looking into how the Go Language and standard library implements go routines synchronization.&lt;/p&gt;

&lt;h3 id=&#34;an-idiomatic-singleton-approach-in-go:dad286903448e7ad7641ef7a6de34a28&#34;&gt;An Idiomatic Singleton Approach in Go&lt;/h3&gt;

&lt;p&gt;We want to implement this Singleton pattern utilizing the Go idiomatic way of doing things. So we have to look at the excellent standard library packaged called &lt;code&gt;sync&lt;/code&gt;. We can find the type &lt;code&gt;Once&lt;/code&gt;. This object will perform an action exactly once and no more. Below you can find the source code from the Go standard library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;// Once is an object that will perform exactly one action.
type Once struct {
	m    Mutex
	done uint32
}

// Do calls the function f if and only if Do is being called for the
// first time for this instance of Once. In other words, given
// 	var once Once
// if once.Do(f) is called multiple times, only the first call will invoke f,
// even if f has a different value in each invocation.  A new instance of
// Once is required for each function to execute.
//
// Do is intended for initialization that must be run exactly once.  Since f
// is niladic, it may be necessary to use a function literal to capture the
// arguments to a function to be invoked by Do:
// 	config.once.Do(func() { config.init(filename) })
//
// Because no call to Do returns until the one call to f returns, if f causes
// Do to be called, it will deadlock.
//
// If f panics, Do considers it to have returned; future calls of Do return
// without calling f.
//
func (o *Once) Do(f func()) {
	if atomic.LoadUint32(&amp;amp;o.done) == 1 { // &amp;lt;-- Check
		return
	}
	// Slow-path.
	o.m.Lock()                           // &amp;lt;-- Lock
	defer o.m.Unlock()
	if o.done == 0 {                     // &amp;lt;-- Check
		defer atomic.StoreUint32(&amp;amp;o.done, 1)
		f()
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this means is the we can leverage the awesome Go sync package to invoke a method exactly only once. Therefore, we can invoke the &lt;code&gt;once.Do()&lt;/code&gt; method like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;once.Do(func() {
    // perform safe initialization here
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below you can find the full code of this singleton implementation that utilizes the &lt;code&gt;sync.Once&lt;/code&gt; type to syncronize access to the &lt;code&gt;GetInstance()&lt;/code&gt; and ensures that our type only gets initialized exactly once.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package singleton

import (
    &amp;quot;sync&amp;quot;
)

type singleton struct {
}

var instance *singleton
var once sync.Once

func GetInstance() *singleton {
    once.Do(func() {
        instance = &amp;amp;singleton{}
    })
    return instance
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Therefore, using the &lt;code&gt;sync.Once&lt;/code&gt; package is the preferred way of implementing this safely, in similar way that Objective-C and Swift (Cocoa) implements the &lt;code&gt;dispatch_once&lt;/code&gt; metod to perform similar initialization.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:dad286903448e7ad7641ef7a6de34a28&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;When it comes to concurrent and parallel code, a lot more careful examination of your code is needed. Always have your team members perform code-reviews, since things like this is easy to have an oversight.&lt;/p&gt;

&lt;p&gt;All the new developers that are switching to Go needs to really understand how thread-safety works to better improve their code. Even though the Go language itself does a lot of the heavy-lifting by allowing you to design concurrent code with minimal knowledge of concurrency. There are several cases where the language doesn&amp;rsquo;t help you, and you still need to apply best practices in developing your code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Supercharging the Atom Editor for Go Development</title>
      <link>http://vaneyckt.io/posts/supercharging-atom-editor-for-go-development/</link>
      <pubDate>Tue, 07 Jul 2015 15:24:59 -0400</pubDate>
      
      <guid>http://vaneyckt.io/posts/supercharging-atom-editor-for-go-development/</guid>
      <description>

&lt;p&gt;After years of using Integrated Development Environments (IDE) during my Windows progamming days, such as Visual Basic IDE, Borland Delphi IDE, Visual C++ and more recent Visual Studio, I have ditched all of those when I switched to Mac OS X about 10 years ago.&lt;/p&gt;

&lt;p&gt;My initial transition in the Mac programming world was using excellent Textmate editor at the time. A fast coding editor with great highlight syntax, extension modules and code snippets that made me feel productive again. After the decline of TextMate several years ago, since the application wasn&amp;rsquo;t getting software updates, a lot of people ended up switching to Sublime Text editor and the traditional VIM editor.&lt;/p&gt;

&lt;p&gt;I have tried Atom when it first came out a while back, but it wasn&amp;rsquo;t ready for prime time. When they officially launched version 1.0 a few days ago, I have decided to give it another try. And I am really happy that I did.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://atom.io/&#34;&gt;https://atom.io/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;installing-a-theme:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Installing a Theme&lt;/h2&gt;

&lt;p&gt;You are going to spend most of your day looking at code and staring at this environment, so you should always find a theme that pleases your eyes and that have a natural color balance. I believe this is very personal, and you should find one that you like the most.&lt;/p&gt;

&lt;p&gt;What I have been using is the combination of the One Dark UI Theme and the Monokai-Seti Syntax theme. I really enjoy the aesthetics of this duo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-theme.png&#34; alt=&#34;atom-theme&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here is how it looks like on my environment:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-editor.png&#34; alt=&#34;atom-editor&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;monokai-seti:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;monokai-seti&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://atom.io/packages/monokai-seti&#34;&gt;https://atom.io/packages/monokai-seti&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-a-programmers-font:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Installing a Programmers Font&lt;/h2&gt;

&lt;p&gt;One of the first things I wanted to do when opening Atom the first time, was to install and use my preferred programming font. I have been using the free &amp;ldquo;Inconsolata&amp;rdquo; font for a while.&lt;/p&gt;

&lt;p&gt;You can find and download it here: &lt;a href=&#34;http://www.levien.com/type/myfonts/inconsolata.html&#34;&gt;http://www.levien.com/type/myfonts/inconsolata.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can easily change the font being used in the editor view through the standard editor settings.&lt;/p&gt;

&lt;h2 id=&#34;installing-languages:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Installing Languages&lt;/h2&gt;

&lt;p&gt;The standard packages that come with Atom cover most of your language needs. There are 2 packages that I have missed for my Go development, one is the support for Dockerfile syntax and Google Protobuf syntax, which both I use in a lot of my projects.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;language-docker: &lt;a href=&#34;https://atom.io/packages/language-docker&#34;&gt;https://atom.io/packages/language-docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;language-protobuf: &lt;a href=&#34;https://atom.io/packages/language-protobuf&#34;&gt;https://atom.io/packages/language-protobuf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;installing-packages:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Installing Packages&lt;/h2&gt;

&lt;h4 id=&#34;go-plus:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;go-plus&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://atom.io/packages/go-plus&#34;&gt;https://atom.io/packages/go-plus&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This package provides almost all of Go Language support in Atom for tools, build flows, linters, vet and coverage tools. It also contains many code snippets and other features.&lt;/p&gt;

&lt;p&gt;Make sure you have all the golang tools installed, by using the following command from your shell:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go get -u golang.org/x/tools/cmd/...
go get -u github.com/golang/lint/golint
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are many features in go-plus. But my favorite while developing my Go code is that I have instant feedback on my syntax and build errors. As soon as I save a file, go-plus runs in the background a miriad of tools like go vet, go oracle, go build, etc, and displays in the bottom of your editor any errors and warnings you may have. This is totally awesome and speeds up your dev cycles dramatically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-go-plus-linter.png&#34; alt=&#34;atom-go-plus-linter&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It has also the ability to display on the editor gutter an indication of any build errors on that line, so you can easily spot which lines has an errors. Errors are displayed in red and warnings in yellowish.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-go-plus-gutter-errors.png&#34; alt=&#34;atom-go-plus-gutter-errors&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;go-rename:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;go-rename&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://atom.io/packages/go-rename&#34;&gt;https://atom.io/packages/go-rename&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This package provides intelligent variable, methods and struct safe renaming by plugging into the Go rename tool. You can easily initiate a rename refactoring dialog by pressing &lt;code&gt;ALT-R&lt;/code&gt; when you have an identifier selected.&lt;/p&gt;

&lt;h2 id=&#34;making-it-a-little-more-similar-to-vim:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Making it a little more similar to VIM&lt;/h2&gt;

&lt;p&gt;You may ask &amp;ldquo;Why not using VIM instead?&amp;rdquo;. The fact is that I have for more than a year, but I wanted to try out Atom, and I am really enjoying the experience, specially since I have all the VIM commands I have been using here in the Atom Editor.&lt;/p&gt;

&lt;p&gt;The fact is that I am a basic VIM user, and most of the benefits for me is about caret navigation, insertion, text replacements, line deletions, line jumps, etc. Once you have installed the nice &lt;code&gt;vim-mode&lt;/code&gt; package by the Atom team, you have all that, and lot more commands that I haven&amp;rsquo;t really used yet from VIM.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;vim-mode: &lt;a href=&#34;https://atom.io/packages/vim-mode&#34;&gt;https://atom.io/packages/vim-mode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;vim-surround: &lt;a href=&#34;https://atom.io/packages/vim-surround&#34;&gt;https://atom.io/packages/vim-surround&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;last-cursor-position: &lt;a href=&#34;https://atom.io/packages/last-cursor-position&#34;&gt;https://atom.io/packages/last-cursor-position&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&#34;mapping-a-few-commands-in-the-keymap-file:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Mapping a few commands in the Keymap File&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&#34;language-cson&#34;&gt;&#39;atom-text-editor:not(mini).autocomplete-active&#39;:
  &#39;ctrl-p&#39;: &#39;core:move-up&#39;
  &#39;ctrl-n&#39;: &#39;core:move-down&#39;

&#39;.vim-mode.command-mode:not(.mini)&#39;:
  &#39;ctrl-f&#39;: &#39;core:page-down&#39;
  &#39;/&#39;: &#39;find-and-replace:show&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One VIM module I miss is Easy-Motion. There was one package for Atom but is currently not compatible with Atom 1.0. I am sure somebody will update or create a version soon.&lt;/p&gt;

&lt;h2 id=&#34;customizing-your-treeview:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Customizing your TreeView&lt;/h2&gt;

&lt;h4 id=&#34;file-icons:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;file-icons&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://atom.io/packages/file-icons&#34;&gt;https://atom.io/packages/file-icons&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At first, I thought this package would make my treeview way too colorful with a bunch of different colored icons for my filetypes in the TreeView, Tabs, and Fuzzy finder dialogs. But I decided to try it out and now I think this is really cool. In fact, it does help me find, at a quick glance, the file I am looking for specially in the root of my project. There are several settings in the package to control whether to show color-less icons and/or to only colorize icons if a file has been modified.&lt;/p&gt;

&lt;p&gt;Below you can see how your TreeView would look like once you have this packaged installed. Give it a try, I am sure you will like it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/file-icons.png&#34; alt=&#34;file-icons&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;applying-some-css-modifications:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Applying some CSS modifications&lt;/h4&gt;

&lt;p&gt;The default line height for the TreeView is usually a little bit too tall, so I wanted to reduce the padding between the lines. Another modification was to use the same fixed font I use in my editor to keep the style consistent. I usually use the &lt;code&gt;Inconsolata&lt;/code&gt; font.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// style the background color of the tree view
.tree-view {
    font-family: &amp;quot;Inconsolata&amp;quot;;
    font-size: 12px;
}

.list-group li:not(.list-nested-item),
.list-tree li:not(.list-nested-item),
.list-group li.list-nested-item &amp;gt; .list-item,
.list-tree li.list-nested-item &amp;gt; .list-item {
    line-height:18px;
}

.list-group .selected:before,
.list-tree .selected:before {
    height:18px;
}

.list-tree.has-collapsable-children .list-nested-item &amp;gt; .list-tree &amp;gt; li,
.list-tree.has-collapsable-children .list-nested-item &amp;gt; .list-group &amp;gt; li {
    padding-left:12px;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;code-snippets:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Code Snippets&lt;/h3&gt;

&lt;p&gt;Most modern programming editors and IDEs comes with code-completion for common keywords and code structures based on the language of choice. It usually contains most of the common ones, but it is far from complete. There lots of rooms for improvements here in the standard stock packages, specially the support for Go via the standard Atom Go langauge package and go-plus package.&lt;/p&gt;

&lt;p&gt;But Atom allows you to create your own code snippets repository in the &lt;code&gt;snippets.cson&lt;/code&gt; file. Open the Snippets file from the Atom menu, and you can start creating your own snippets library.&lt;/p&gt;

&lt;p&gt;You will have to create your entries under a &lt;code&gt;.source.go&lt;/code&gt; scope, so your snippets will only be suggested by the auto-completion feature when you are editing Go files.&lt;/p&gt;

&lt;p&gt;Here are a few that I have added recently:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-cson&#34;&gt;&#39;.source.go&#39;:
    &#39;return nil and error&#39;:
      &#39;prefix&#39;: &#39;rne&#39;
      &#39;body&#39;: &#39;return nil, err&#39;

    &#39;return false and error&#39;:
      &#39;prefix&#39;: &#39;rfe&#39;
      &#39;body&#39;: &#39;return false, err&#39;

    &#39;Return True and Nil&#39;:
      &#39;prefix&#39;: &#39;rte&#39;
      &#39;body&#39;: &#39;return true, nil&#39;

    &#39;Import logrus&#39;:
      &#39;prefix&#39;: &#39;logrus&#39;
      &#39;body&#39;: &#39;log &amp;quot;github.com/Sirupsen/logrus&amp;quot;&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have defined your snippets, they immediately show up in your auto-completion suggestions. I love this feature, and I try to add common snippets of my Go code to really speed up typing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-complete-snippets.png&#34; alt=&#34;atom-complete-snippets&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;dash:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Dash&lt;/h3&gt;

&lt;p&gt;Dash is a fantastic commercial application for Mac OS X that gives you instant offline access to 150+ API documentation sets from many languages and frameworks. I have been using it for a few years with Ruby development and now with Go.&lt;/p&gt;

&lt;p&gt;You can find more about it here: &lt;a href=&#34;https://kapeli.com/dash&#34;&gt;https://kapeli.com/dash&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-dash.png&#34; alt=&#34;atom-dash&#34; /&gt;&lt;/p&gt;

&lt;p&gt;There is an Atom package that integrates your editor shortcuts via &lt;code&gt;CTRL-H&lt;/code&gt; once you have a text selected and opens up directly in the Dash application. Pretty hand when you want to quickly jumpt to a method declaration documentation.&lt;/p&gt;

&lt;h4 id=&#34;dash-1:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;dash&lt;/h4&gt;

&lt;p&gt;&lt;a href=&#34;https://atom.io/packages/dash&#34;&gt;https://atom.io/packages/dash&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;styling-your-editor:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Styling your Editor&lt;/h2&gt;

&lt;p&gt;One of the biggest advantages of Atom editor compared to other native editors, is its ability to completely customize the interface using Cascade Stylesheets (CSS). Almost all aspects of the editor can be tweaked and improved if it doesn&amp;rsquo;t appeal you.&lt;/p&gt;

&lt;h3 id=&#34;changing-symbol-view-appearance:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Changing Symbol View appearance&lt;/h3&gt;

&lt;p&gt;One of the areas that really bothered me was the stock Symbols-View dialog, the lines were too tall and could barely fit many rows in the lookup window. I liked the way Sublime implemented and I decided to customize it to resembles more of what Sublime had.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;.symbols-view {
  &amp;amp;.select-list ol.list-group li .primary-line,
  &amp;amp;.select-list ol.list-group li .secondary-line {

    font-family: Inconsolata;
    font-size: 14px;

    // let lines wrap
    text-overflow: initial;
    white-space: initial;
    overflow: initial;

    // reduce line-height
    line-height: 1.0em;
    padding-top: .1em;
    padding-bottom: .0em;

    // make sure wrapped lines get padding
    // padding-left: 21px;
    &amp;amp;.icon:before {
      margin-left: -21px;
    }

    .character-match {
        color: rgb(200, 200, 10) !important;
    }
  }
}

.symbols-view {
  &amp;amp;.select-list ol.list-group li .secondary-line {
      float: right;
      margin-top: -12px;
      padding-top: 0em;
      padding-bottom: 0em;
      font-size: 12px;
      color: rgba(200, 200, 10, 0.8) !important;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The end result is a much slicker Symbols-View. When you press &lt;code&gt;Command+R&lt;/code&gt; the symbols view appear, and you can see that line-heights are shorter and the line numbers aligned to the right of the view.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-symbol-view-styles.png&#34; alt=&#34;atom-symbol-view-styles&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;styling-line-selection:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Styling Line Selection&lt;/h3&gt;

&lt;p&gt;There is an interesting package called Hightlight-Line:&lt;br /&gt;
&lt;a href=&#34;https://atom.io/packages/highlight-line&#34;&gt;https://atom.io/packages/highlight-line&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This package allows customization of the line selection styles. In my case, I have added a dashed yellow border to the bottom and top my selection. I like the way it looks and helps me determine the range of selection specially at the last line where it could be a partial selection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-highlight-selected.png&#34; alt=&#34;atom-highlight-selected&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can replace &amp;lsquo;solid&amp;rsquo;, with &amp;lsquo;dashed&amp;rsquo; or &amp;lsquo;dotted&amp;rsquo; on the CSS selector depending of what you have
set in the package settings page.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;atom-text-editor::shadow {

    .line.highlight-line {
        background: rgba(255, 255, 255, 0.05) !important;
    }

    .line.highlight-line-multi-line-dashed-bottom {
        border-bottom-color: yellow !important;
    }

    .line.highlight-line-multi-line-dashed-top {
        border-top-color: yellow !important;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;generating-ctags-for-your-go-code:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Generating Ctags for your Go code&lt;/h2&gt;

&lt;p&gt;Auto-completion is a very important feature when programming, and is usually very intrusive to the developers. So the suggestions it should make needs to be very good or it will annoy you really quick. Missing suggestions is one of the most frustating things when it comes to auto-completion functionality.&lt;/p&gt;

&lt;p&gt;When you know a structure, interface or method name you want to use the Auto-completion so you can type fast and jump to the next work. When you are constantly not finding the items that should be there your developer happiness level suffers tremendously.&lt;/p&gt;

&lt;p&gt;There is an awesome tool called &lt;code&gt;gotags&lt;/code&gt; that is ctags compatible generator for Go Language. It utilizes the power of AST and Parsing classes in the Go standard library to really understand and capture all the structure, interfaces, variables and methods names. It generates a much better ctags list than the standard ctags standard tools.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gotags: &lt;a href=&#34;https://github.com/jstemmer/gotags&#34;&gt;https://github.com/jstemmer/gotags&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can install it by running the following commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go get -u github.com/jstemmer/gotags
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then generate tags simply by invoking it like this from the root of your source code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gotags -tag-relative=true -R=true -sort=true -f=&amp;quot;tags&amp;quot; -fields=+l .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate a new tags file in the root of your code, and Atom auto-completion will be much more smarter about your Go code.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/atom-go-tags.png&#34; alt=&#34;atom-go-tags&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion:0386400aabdb04ed1ae1687fb75a55b9&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Whether you like Sublime Text or have been a VIM fanatic, you should give Atom a shot now that they have reached version 1.0 a couple of weeks ago.&lt;/p&gt;

&lt;p&gt;Especially since it is an open-source project backed by Github.com, there is a lot of activity and the community is growing incredibly fast.&lt;/p&gt;

&lt;p&gt;Give it a try !!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Calculating Multiple File Hashes in a Single Pass</title>
      <link>http://vaneyckt.io/posts/calculating-multiple-file-hashes-in-a-single-pass/</link>
      <pubDate>Tue, 07 Jul 2015 11:14:06 -0400</pubDate>
      
      <guid>http://vaneyckt.io/posts/calculating-multiple-file-hashes-in-a-single-pass/</guid>
      <description>

&lt;p&gt;We do a lot of file hash calculations at work, where we commonly go through millions of files a day using a diverse number of hashing algorithms. The standard Go library is amazing, and it has many structures and methods to do all that kind of stuff. Sometimes you just have to look for some new methods that opens up the possibilities even more.&lt;/p&gt;

&lt;h3 id=&#34;the-goal:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;The Goal&lt;/h3&gt;

&lt;p&gt;The initial goal of this code was to calculate multiple hashes on a single file residing on disk, and only perform a single read, instead of reading the whole contents of the file multiple times for each hash algorithm.&lt;/p&gt;

&lt;p&gt;The idea was to return an structure with the results of the desired hash:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type HashInfo struct {
	Md5    string `json:&amp;quot;md5&amp;quot;`
	Sha1   string `json:&amp;quot;sha1&amp;quot;`
	Sha256 string `json:&amp;quot;sha256&amp;quot;`
	Sha512 string `json:&amp;quot;sha512&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;looking-into-the-standard-library:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;Looking into the standard library&lt;/h3&gt;

&lt;p&gt;As park of the Go standard library &lt;code&gt;io&lt;/code&gt; package, we can find this function below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func MultiWriter(writers ...Writer) Writer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is a snippet of the source code implementation of this function in the &lt;code&gt;io&lt;/code&gt; package:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type multiWriter struct {
    writers []Writer
}

func (t *multiWriter) Write(p []byte) (n int, err error) {
	for _, w := range t.writers {
		n, err = w.Write(p)
		if err != nil {
			return
		}
		if n != len(p) {
			err = ErrShortWrite
			return
		}
	}
	return len(p), nil
}

func MultiWriter(writers ...Writer) Writer {
	w := make([]Writer, len(writers))
	copy(w, writers)
	return &amp;amp;multiWriter{w}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The MultiWriter method creates a writer that duplicates its writes to all the provided writers, similar to the Unix &lt;code&gt;tee&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;This is interesting because since all of hash functions in standard library adheres to this interface:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Hash interface {
        // Write (via the embedded io.Writer interface) adds more data to the running hash.
        // It never returns an error.
        io.Writer

        // Sum appends the current hash to b and returns the resulting slice.
        // It does not change the underlying hash state.
        Sum(b []byte) []byte

        // Reset resets the Hash to its initial state.
        Reset()

        // Size returns the number of bytes Sum will return.
        Size() int

        // BlockSize returns the hash&#39;s underlying block size.
        // The Write method must be able to accept any amount
        // of data, but it may operate more efficiently if all writes
        // are a multiple of the block size.
        BlockSize() int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-approach:87d0f12486ddffd4cd2e61232b39b4f8&#34;&gt;The Approach&lt;/h3&gt;

&lt;p&gt;Therefore, we could create a &lt;code&gt;MultiWriter&lt;/code&gt; that is going to write to multiple Hash implementations only performing a single read pass through the original file, as you can see in the code below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func CalculateBasicHashes(rd io.Reader) HashInfo {

	md5 := md5.New()
	sha1 := sha1.New()
	sha256 := sha256.New()
	sha512 := sha512.New()

	// For optimum speed, Getpagesize returns the underlying system&#39;s memory page size.
	pagesize := os.Getpagesize()

	// wraps the Reader object into a new buffered reader to read the files in chunks
	// and buffering them for performance.
	reader := bufio.NewReaderSize(rd, pagesize)

	// creates a multiplexer Writer object that will duplicate all write
	// operations when copying data from source into all different hashing algorithms
	// at the same time
	multiWriter := io.MultiWriter(md5, sha1, sha256, sha512)

	// Using a buffered reader, this will write to the writer multiplexer
	// so we only traverse through the file once, and can calculate all hashes
	// in a single byte buffered scan pass.
	//
	_, err := io.Copy(multiWriter, reader)
	if err != nil {
		panic(err.Error())
	}

	var info HashInfo

	info.Md5 = hex.EncodeToString(md5.Sum(nil))
	info.Sha1 = hex.EncodeToString(sha1.Sum(nil))
	info.Sha256 = hex.EncodeToString(sha256.Sum(nil))
	info.Sha512 = hex.EncodeToString(sha512.Sum(nil))

	return info
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is a sample of command line utility to calculate the multiple hashes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
	&amp;quot;bufio&amp;quot;
	&amp;quot;crypto/md5&amp;quot;
	&amp;quot;crypto/sha1&amp;quot;
	&amp;quot;crypto/sha256&amp;quot;
	&amp;quot;crypto/sha512&amp;quot;
	&amp;quot;encoding/hex&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;io&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;runtime&amp;quot;
)

func main() {
	args := os.Args[1:]

	var filename string
	filename = args[0]

    // open an io.Reader from the file we would like to calculate hashes
	f, err := os.OpenFile(filename, os.O_RDONLY, 0)
	if err != nil {
		log.Fatalln(&amp;quot;Cannot open file: %s&amp;quot;, filename)
	}
	defer f.Close()

	info := CalculateBasicHashes(f)

	fmt.Println(&amp;quot;md5    :&amp;quot;, info.Md5)
	fmt.Println(&amp;quot;sha1   :&amp;quot;, info.Sha1)
	fmt.Println(&amp;quot;sha256 :&amp;quot;, info.Sha256)
	fmt.Println(&amp;quot;sha512 :&amp;quot;, info.Sha512)
	fmt.Println()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course that in a real-world scenario we wouldn&amp;rsquo;t be invoking the command line utility for every single file. This was just a simple example on how to write a little command line utility to demonstrate this approach. The real benefit is when we are traversing through millions of files and performing hash calculations using a single read pass through the contents file. This has a significant impact on our ability to fast go through our file repositories.&lt;/p&gt;

&lt;p&gt;There are so many interesting functions and interfaces in the standard library that everyone should take look at the source code once in a while.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Handling 1 Million Requests per Minute with Go</title>
      <link>http://vaneyckt.io/posts/handling-1-million-requests-per-minute-with-golang/</link>
      <pubDate>Mon, 06 Jul 2015 16:49:35 -0400</pubDate>
      
      <guid>http://vaneyckt.io/posts/handling-1-million-requests-per-minute-with-golang/</guid>
      <description>

&lt;p&gt;Here at &lt;a href=&#34;http://www.malwarebytes.org&#34;&gt;Malwarebytes&lt;/a&gt; we are experiencing phenomenal growth, and since I have joined the company over 1 year ago in the Silicon Valley, one my main responsibilities has been to architect and develop several systems to power a fast-growing security company and all the needed infrastructure to support a product that is used by millions of people every single day. I have worked in the anti-virus and anti-malware industry for over 12 years in a few different companies, and I knew how complex these systems could end up being due to the massive amount of data we handle daily.&lt;/p&gt;

&lt;p&gt;What is interesting is that for the last 9 years or so, all the web backend development that I have been involved in has been mostly done in Ruby on Rails. Don&amp;rsquo;t take me wrong, I love Ruby on Rails and I believe it&amp;rsquo;s an amazing environment, but after a while you start thinking and designing systems in the ruby way, and you forget how efficient and simple your software architecture could have been if you could leverage multi-threading, parallelization, fast executions and small memory overhead. For many years, I was a C/C++, Delphi and C# developer, and I just started realizing how less complex things could be with the right tool for the job.&lt;/p&gt;

&lt;p&gt;As a Principal Architect, I am not very big on the language and framework wars that the interwebs are always fighting about. I believe efficiency, productivity and code maintainability relies mostly on how simple you can architect your solution.&lt;/p&gt;

&lt;h2 id=&#34;the-problem:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Problem&lt;/h2&gt;

&lt;p&gt;While working on a piece of our anonymous telemetry and analytics system, our goal was to be able to handle a large amount of POST requests from millions of endpoints. The web handler would receive a JSON document that may contain a collection of many payloads that needed to be written to Amazon S3, in order for our map-reduce systems to later operate on this data.&lt;/p&gt;

&lt;p&gt;Traditionally we would look into creating a worker-tier architecture, utilizing things such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sidekiq&lt;/li&gt;
&lt;li&gt;Resque&lt;/li&gt;
&lt;li&gt;DelayedJob&lt;/li&gt;
&lt;li&gt;Elasticbeanstalk Worker Tier&lt;/li&gt;
&lt;li&gt;RabbitMQ&lt;/li&gt;
&lt;li&gt;and so on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And setup 2 different clusters, one for the web front-end and another for the workers, so we can scale up the amount of background work we can handle.&lt;/p&gt;

&lt;p&gt;But since the beginning, our team knew that we should do this in Go because during the discussion phases we saw this could be potentially a very large traffic system. I have been using Go for about 2 years or so, and we had developed a few systems here at work but none that would get this amount of load.&lt;/p&gt;

&lt;p&gt;We started by creating a few structures to define the web request payload that we would be receiving through the POST calls, and a method to upload it into our S3 bucket.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type PayloadCollection struct {
	WindowsVersion  string    `json:&amp;quot;version&amp;quot;`
	Token           string    `json:&amp;quot;token&amp;quot;`
	Payloads        []Payload `json:&amp;quot;data&amp;quot;`
}

type Payload struct {
    // [redacted]
}

func (p *Payload) UploadToS3() error {
    // the storageFolder method ensures that there are no name collision in
    // case we get same timestamp in the key name
    storage_path := fmt.Sprintf(&amp;quot;%v/%v&amp;quot;, p.storageFolder, time.Now().UnixNano())

	bucket := S3Bucket

	b := new(bytes.Buffer)
	encodeErr := json.NewEncoder(b).Encode(payload)
	if encodeErr != nil {
		return encodeErr
	}

    // Everything we post to the S3 bucket should be marked &#39;private&#39;
    var acl = s3.Private
	var contentType = &amp;quot;application/octet-stream&amp;quot;

	return bucket.PutReader(storage_path, b, int64(b.Len()), contentType, acl, s3.Options{})
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;naive-approach-to-go-routines:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Naive approach to Go routines&lt;/h3&gt;

&lt;p&gt;Initially we took a very naive implementation of the POST handler, just trying to parallelize the job processing into a simple goroutine:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func payloadHandler(w http.ResponseWriter, r *http.Request) {

    if r.Method != &amp;quot;POST&amp;quot; {
		w.WriteHeader(http.StatusMethodNotAllowed)
		return
	}

    // Read the body into a string for json decoding
	var content = &amp;amp;PayloadCollection{}
	err := json.NewDecoder(io.LimitReader(r.Body, MaxLength)).Decode(&amp;amp;content)
    if err != nil {
		w.Header().Set(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json; charset=UTF-8&amp;quot;)
		w.WriteHeader(http.StatusBadRequest)
		return
	}

    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {
        go payload.UploadToS3()   // &amp;lt;----- DON&#39;T DO THIS
    }

    w.WriteHeader(http.StatusOK)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For moderate loads, this could work for the majority of people, but this quickly proved to not work very well at a large scale. We were expecting a lot of requests but not in the order of magnitude we started seeing when we deployed the first version to production. We completely understimated the amount of traffic.&lt;/p&gt;

&lt;p&gt;The approach above is bad in several different ways. There is no way to control how many go routines we are spawning. And since we were getting 1 million POST requests per minute of course this code crashed and burned very quickly.&lt;/p&gt;

&lt;h3 id=&#34;trying-again:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Trying again&lt;/h3&gt;

&lt;p&gt;We needed to find a different way. Since the beginning we started discussing how we needed to keep the lifetime of the request handler very short and spawn processing in the background. Of course, this is what you must do in the Ruby on Rails world, otherwise you will block all the available worker web processors, whether you are using puma, unicorn, passenger (Let&amp;rsquo;s not get into the JRuby discussion please). Then we would have needed to leverage common solutions to do this, such as Resque, Sidekiq, SQS, etc. The list goes on since there are many ways of achieving this.&lt;/p&gt;

&lt;p&gt;So the second iteration was to create a buffered channel where we could queue up some jobs and upload them to S3, and since we could control the maximum number of items in our queue and we had plenty of RAM to queue up jobs in memory, we thought it would be okay to just buffer jobs in the channel queue.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var Queue chan Payload

func init() {
    Queue = make(chan Payload, MAX_QUEUE)
}

func payloadHandler(w http.ResponseWriter, r *http.Request) {
    ...
    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {
        Queue &amp;lt;- payload
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then to actually dequeue jobs and process them, we were using something similar to this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func StartProcessor() {
    for {
        select {
        case job := &amp;lt;-Queue:
            job.payload.UploadToS3()  // &amp;lt;-- STILL NOT GOOD
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To be honest, I have no idea what we were thinking. This must have been a late night full of Red-Bulls. This approach didn&amp;rsquo;t buy us anything, we have traded flawed concurrency with a buffered queue that was simply postponing the problem. Our synchronous processor was only uploading one payload at a time to S3, and since the rate of incoming requests were much larger than the ability of the single processor to upload to S3, our buffered channel was quickly reaching its limit and blocking the request handler ability to queue more items.&lt;/p&gt;

&lt;p&gt;We were simply avoiding the problem and started a count-down to the death of our system eventually. Our latency rates kept increasing in a constant rate minutes after we deployed this flawed version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/cloudwatch-latency.png&#34; alt=&#34;cloudwatch-latency&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-better-solution:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Better Solution&lt;/h3&gt;

&lt;p&gt;We have decided to utilize a common pattern when using Go channels, in order to create a 2-tier channel system, one for queuing jobs and another to control how many workers operate on the JobQueue concurrently.&lt;/p&gt;

&lt;p&gt;The idea was to parallelize the uploads to S3 to a somewhat sustainable rate, one that would not cripple the machine nor start generating connections errors from S3. So we have opted for creating a Job/Worker pattern. For those that are familiar with Java, C#, etc, think about this as the Golang way of implementing a Worker Thread-Pool utilizing channels instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var (
	MaxWorker = os.Getenv(&amp;quot;MAX_WORKERS&amp;quot;)
	MaxQueue  = os.Getenv(&amp;quot;MAX_QUEUE&amp;quot;)
)

// Job represents the job to be run
type Job struct {
	Payload Payload
}

// A buffered channel that we can send work requests on.
var JobQueue chan Job

// Worker represents the worker that executes the job
type Worker struct {
	WorkerPool  chan chan Job
	JobChannel  chan Job
	quit    	chan bool
}

func NewWorker(workerPool chan chan Job) Worker {
	return Worker{
		WorkerPool: workerPool,
		JobChannel: make(chan Job),
		quit:       make(chan bool)}
}

// Start method starts the run loop for the worker, listening for a quit channel in
// case we need to stop it
func (w Worker) Start() {
	go func() {
		for {
			// register the current worker into the worker queue.
			w.WorkerPool &amp;lt;- w.JobChannel

			select {
			case job := &amp;lt;-w.JobChannel:
				// we have received a work request.
				if err := job.Payload.UploadToS3(); err != nil {
					log.Errorf(&amp;quot;Error uploading to S3: %s&amp;quot;, err.Error())
				}

			case &amp;lt;-w.quit:
				// we have received a signal to stop
				return
			}
		}
	}()
}

// Stop signals the worker to stop listening for work requests.
func (w Worker) Stop() {
	go func() {
		w.quit &amp;lt;- true
	}()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We have modified our Web request handler to create an instance of &lt;code&gt;Job&lt;/code&gt; struct with the payload and send into the &lt;code&gt;JobQueue&lt;/code&gt; channel for the workers to pickup.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func payloadHandler(w http.ResponseWriter, r *http.Request) {

    if r.Method != &amp;quot;POST&amp;quot; {
		w.WriteHeader(http.StatusMethodNotAllowed)
		return
	}

    // Read the body into a string for json decoding
	var content = &amp;amp;PayloadCollection{}
	err := json.NewDecoder(io.LimitReader(r.Body, MaxLength)).Decode(&amp;amp;content)
    if err != nil {
		w.Header().Set(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json; charset=UTF-8&amp;quot;)
		w.WriteHeader(http.StatusBadRequest)
		return
	}

    // Go through each payload and queue items individually to be posted to S3
    for _, payload := range content.Payloads {

        // let&#39;s create a job with the payload
        work := Job{Payload: payload}

        // Push the work onto the queue.
        JobQueue &amp;lt;- work
    }

    w.WriteHeader(http.StatusOK)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During our web server initialization we create a &lt;code&gt;Dispatcher&lt;/code&gt; and call &lt;code&gt;Run()&lt;/code&gt; to create the pool of workers and to start listening for jobs that would appear in the &lt;code&gt;JobQueue&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;dispatcher := NewDispatcher(MaxWorker)
dispatcher.Run()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is the code for our dispatcher implementation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;type Dispatcher struct {
	// A pool of workers channels that are registered with the dispatcher
	WorkerPool chan chan Job
}

func NewDispatcher(maxWorkers int) *Dispatcher {
	pool := make(chan chan Job, maxWorkers)
	return &amp;amp;Dispatcher{WorkerPool: pool}
}

func (d *Dispatcher) Run() {
    // starting n number of workers
	for i := 0; i &amp;lt; d.maxWorkers; i++ {
		worker := NewWorker(d.pool)
		worker.Start()
	}

	go d.dispatch()
}

func (d *Dispatcher) dispatch() {
	for {
		select {
		case job := &amp;lt;-JobQueue:
			// a job request has been received
			go func(job Job) {
				// try to obtain a worker job channel that is available.
				// this will block until a worker is idle
				jobChannel := &amp;lt;-d.WorkerPool

				// dispatch the job to the worker job channel
				jobChannel &amp;lt;- job
			}(job)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we provide the number of maximum workers to be instantiated and be added to our pool of workers. Since we have utilized Amazon Elasticbeanstalk for this project with a dockerized Go environment, and we always try to follow the &lt;a href=&#34;http://12factor.net/&#34;&gt;12-factor&lt;/a&gt; methodology to configure our systems in production, we read these values from environment variables. That way we could control how many workers and the maximum size of the Job Queue, so we can quickly tweak these values without requiring re-deployment of the cluster.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var (
	MaxWorker = os.Getenv(&amp;quot;MAX_WORKERS&amp;quot;)
	MaxQueue  = os.Getenv(&amp;quot;MAX_QUEUE&amp;quot;)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-immediate-results:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;The Immediate results&lt;/h3&gt;

&lt;p&gt;Immediately after we have deployed it we saw all of our latency rates drop to insignificant numbers and our ability to handle requests surged drastically.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/cloudwatch-console.png&#34; alt=&#34;cloudwatch-console&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Minutes after our Elastic Load Balancers were fully warmed up, we saw our ElasticBeanstalk application serving close to 1 million requests per minute. We usually have a few hours during the morning hours in which our traffic spikes over to more than a million per minute.&lt;/p&gt;

&lt;p&gt;As soon as we have deployed the new code, the number of servers dropped considerably from 100 servers to about 20 servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/elasticbeanstalk-healthy-hosts.png&#34; alt=&#34;elasticbeanstalk-healthy-hosts&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After we had properly configured our cluster and the auto-scaling settings, we were able to lower it even more to only 4x EC2 c4.Large instances and the Elastic Auto-Scaling set to spawn a new instance if CPU goes above 90% for 5 minutes straight.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://vaneyckt.io/img/elasticbeanstalk-production-dashboard.png&#34; alt=&#34;elasticbeanstalk-production-dashboard&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion:62be2ce69dda4f10948645c26c4bbfee&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Simplicity always wins in my book. We could have designed a complex system with many queues, background workers, complex deployments, but instead we decided to leverage the power of Elasticbeanstalk auto-scaling and the efficiency and simple approach to concurrency that Golang provides us out of the box.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s not everyday that you have a cluster of only 4 machines, that are probably much less powerful than my current MacBook Pro, handling POST requests writing to an Amazon S3 bucket 1 million times every minute.&lt;/p&gt;

&lt;p&gt;There is always the right tool for the job. For sometimes when your Ruby on Rails system needs a very powerful web handler, think a little outside of the ruby eco-system for simpler yet more powerful alternative solutions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Profiling rails assets precompilation</title>
      <link>http://vaneyckt.io/posts/profiling_rails_assets_precompilation/</link>
      <pubDate>Sun, 01 Sep 2013 21:01:02 +0000</pubDate>
      
      <guid>http://vaneyckt.io/posts/profiling_rails_assets_precompilation/</guid>
      <description>&lt;p&gt;Assets precompilation on rails can take a fair bit of time. This is especially annoying in scenarios where you want to deploy your app multiple times a day. Let&amp;rsquo;s see if we can come up with a way to actually figure out where all this time is being spent. Also, while I will be focusing on rails 3.2 in this post, the general principle should be easy enough to apply to other rails versions.&lt;/p&gt;

&lt;p&gt;Our first call of action is finding the assets precompilation logic. A bit of digging will turn up the &lt;a href=&#34;https://github.com/rails/rails/blob/3-2-stable/actionpack/lib/sprockets/assets.rake&#34;&gt;assets.rake file&lt;/a&gt; for rails 3.2. The relevant code starts on lines 59-67 and from there on out invokes methods throughout the entire file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# lines 59-67 of assets.rake
task :all do
  Rake::Task[&amp;quot;assets:precompile:primary&amp;quot;].invoke
  # We need to reinvoke in order to run the secondary digestless
  # asset compilation run - a fresh Sprockets environment is
  # required in order to compile digestless assets as the
  # environment has already cached the assets on the primary
  # run.
  if Rails.application.config.assets.digest
    ruby_rake_task(&amp;quot;assets:precompile:nondigest&amp;quot;, false)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When we follow the calls made by the code above we can see that the actual compilation takes place on lines 50-56 of assets.rake and is done by the compile method of the &lt;a href=&#34;https://github.com/rails/rails/blob/3-2-stable/actionpack/lib/sprockets/static_compiler.rb&#34;&gt;Sprockets::StaticCompiler class&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# compile method of Sprockets::StaticCompiler class
def compile
  manifest = {}
  env.each_logical_path(paths) do |logical_path|
    if asset = env.find_asset(logical_path)
      digest_path = write_asset(asset)
      manifest[asset.logical_path] = digest_path
      manifest[aliased_path_for(asset.logical_path)] = digest_path
    end
  end
  write_manifest(manifest) if @manifest
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we know which code does the compiling, we can think of two ways to add some profiling to this. We could checkout the rails repo from Github, modify it locally and point our Gemfile to our modified local version of rails. Or, we could create a new rake task and monkey patch the compile method of the Sprockets::StaticCompiler class. We&amp;rsquo;ll go with the second option here as it is the more straightforward to implement.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll create a new rake file in the /lib/tasks folder of our rails app and name it &lt;code&gt;profile_assets_precompilation.rake&lt;/code&gt;. We then copy the contents of assets.rake into it, and wrap this code inside a new &amp;lsquo;profile&amp;rsquo; namespace so as to avoid conflicts. At the top of this file we&amp;rsquo;ll also add our monkey patched compile method so as to make it output profiling info. The resulting file should look like shown below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;namespace :profile do
  # monkey patch the compile method to output compilation times
  module Sprockets
    class StaticCompiler
      def compile
        manifest = {}
        env.each_logical_path(paths) do |logical_path|
          start_time = Time.now.to_f

          if asset = env.find_asset(logical_path)
            digest_path = write_asset(asset)
            manifest[asset.logical_path] = digest_path
            manifest[aliased_path_for(asset.logical_path)] = digest_path
          end

          # our profiling code
          duration = Time.now.to_f - start_time
          puts &amp;quot;#{logical_path} - #{duration.round(3)} seconds&amp;quot;
        end
        write_manifest(manifest) if @manifest
      end
    end
  end

  # contents of assets.rake
  namespace :assets do
    def ruby_rake_task(task, fork = true)
      env    = ENV[&#39;RAILS_ENV&#39;] || &#39;production&#39;
      groups = ENV[&#39;RAILS_GROUPS&#39;] || &#39;assets&#39;
      args   = [$0, task,&amp;quot;RAILS_ENV=#{env}&amp;quot;,&amp;quot;RAILS_GROUPS=#{groups}&amp;quot;]
      args &amp;lt;&amp;lt; &amp;quot;--trace&amp;quot; if Rake.application.options.trace
      if $0 =~ /rake\.bat\Z/i
        Kernel.exec $0, *args
      else
        fork ? ruby(*args) : Kernel.exec(FileUtils::RUBY, *args)
      end
    end

    # We are currently running with no explicit bundler group
    # and/or no explicit environment - we have to reinvoke rake to
    # execute this task.
    def invoke_or_reboot_rake_task(task)
      if ENV[&#39;RAILS_GROUPS&#39;].to_s.empty? || ENV[&#39;RAILS_ENV&#39;].to_s.empty?
        ruby_rake_task task
      else
        Rake::Task[task].invoke
      end
    end

    desc &amp;quot;Compile all the assets named in config.assets.precompile&amp;quot;
    task :precompile do
      invoke_or_reboot_rake_task &amp;quot;assets:precompile:all&amp;quot;
    end

    namespace :precompile do
      def internal_precompile(digest=nil)
        unless Rails.application.config.assets.enabled
          warn &amp;quot;Cannot precompile assets if sprockets is disabled. Please set config.assets.enabled to true&amp;quot;
          exit
        end

        # Ensure that action view is loaded and the appropriate
        # sprockets hooks get executed
        _ = ActionView::Base

        config = Rails.application.config
        config.assets.compile = true
        config.assets.digest  = digest unless digest.nil?
        config.assets.digests = {}

        env      = Rails.application.assets
        target   = File.join(Rails.public_path, config.assets.prefix)
        compiler = Sprockets::StaticCompiler.new(env,
                                                 target,
                                                 config.assets.precompile,
                                                 :manifest_path =&amp;gt; config.assets.manifest,
                                                 :digest =&amp;gt; config.assets.digest,
                                                 :manifest =&amp;gt; digest.nil?)
        compiler.compile
      end

      task :all do
        Rake::Task[&amp;quot;assets:precompile:primary&amp;quot;].invoke
        # We need to reinvoke in order to run the secondary digestless
        # asset compilation run - a fresh Sprockets environment is
        # required in order to compile digestless assets as the
        # environment has already cached the assets on the primary
        # run.
        ruby_rake_task(&amp;quot;assets:precompile:nondigest&amp;quot;, false) if Rails.application.config.assets.digest
      end

      task :primary =&amp;gt; [&amp;quot;assets:environment&amp;quot;, &amp;quot;tmp:cache:clear&amp;quot;] do
        internal_precompile
      end

      task :nondigest =&amp;gt; [&amp;quot;assets:environment&amp;quot;, &amp;quot;tmp:cache:clear&amp;quot;] do
        internal_precompile(false)
      end
    end

    desc &amp;quot;Remove compiled assets&amp;quot;
    task :clean do
      invoke_or_reboot_rake_task &amp;quot;assets:clean:all&amp;quot;
    end

    namespace :clean do
      task :all =&amp;gt; [&amp;quot;assets:environment&amp;quot;, &amp;quot;tmp:cache:clear&amp;quot;] do
        config = Rails.application.config
        public_asset_path = File.join(Rails.public_path, config.assets.prefix)
        rm_rf public_asset_path, :secure =&amp;gt; true
      end
    end

    task :environment do
      if Rails.application.config.assets.initialize_on_precompile
        Rake::Task[&amp;quot;environment&amp;quot;].invoke
      else
        Rails.application.initialize!(:assets)
        Sprockets::Bootstrap.new(Rails.application).run
      end
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can run &lt;code&gt;bundle exec rake profile:assets:precompile&lt;/code&gt; to precompile our assets while outputting profiling info. Hopefully we can now finally figure out why this is always taking so long :).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regarding if statement scope in Ruby</title>
      <link>http://vaneyckt.io/posts/regarding_if_statement_scope_in_ruby/</link>
      <pubDate>Sat, 31 Aug 2013 20:22:58 +0000</pubDate>
      
      <guid>http://vaneyckt.io/posts/regarding_if_statement_scope_in_ruby/</guid>
      <description>&lt;p&gt;I recently learned that &lt;code&gt;if&lt;/code&gt; statements in Ruby do not introduce scope. This means that you can write code like shown below and it&amp;rsquo;ll work fine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# perfectly valid Ruby code
if true
  foo = 5
end

puts foo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;At first this seemed a bit weird to me. It wasn&amp;rsquo;t until I read &lt;a href=&#34;http://programmers.stackexchange.com/questions/58900/why-if-statements-do-not-introduce-scope-in-ruby-1-9&#34;&gt;this&lt;/a&gt; that I realized Ruby was even more versatile than I had first thought. As it turns out, it is this somewhat unconventional scoping rule that allows us to conditionally replace methods.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;if foo == 5
  def some_method
    # do something
  end
else
  def some_method
    # do something else
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As well as conditionally modify implementations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;if foo == 5
  class someClass
    # ...
  end
else
  module someModule
    # ...
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s amazing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EC2 instance cost comparison</title>
      <link>http://vaneyckt.io/posts/ec2_instance_cost_comparison/</link>
      <pubDate>Sun, 11 Aug 2013 21:24:12 +0000</pubDate>
      
      <guid>http://vaneyckt.io/posts/ec2_instance_cost_comparison/</guid>
      <description>&lt;p&gt;Amazon&amp;rsquo;s pricing scheme for its ec2 instances never struck me as particularly transparent. Until recently some of my DevOps colleagues even estimated cost by cross-referencing &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types&#34;&gt;instance details&lt;/a&gt; with &lt;a href=&#34;http://aws.amazon.com/ec2/pricing&#34;&gt;pricing information&lt;/a&gt;. While this approach gives reasonable results for finding the cost of a given instance type, it doesn&amp;rsquo;t lend itself very well to comparing prices across a range of different types.&lt;/p&gt;

&lt;p&gt;When talking to an ex-colleague of mine about the hardships encountered for such a common task, he pointed me to &lt;a href=&#34;http://www.ec2instances.info&#34;&gt;this absolutely brilliant page&lt;/a&gt;. It&amp;rsquo;s so unbelievably simple and well thought-out that I can&amp;rsquo;t help getting ever so slightly annoyed with whomever is in charge of communicating Amazon&amp;rsquo;s pricing structure and the subpar job they are doing.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>