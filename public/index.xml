<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vaneyckt.io</title>
    <link>https://vaneyckt.io/</link>
    <description>Recent content on vaneyckt.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Tom Van Eyck</copyright>
    <lastBuildDate>Thu, 17 Mar 2016 19:12:21 +0000</lastBuildDate>
    <atom:link href="https://vaneyckt.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Ruby concurrency: in praise of the mutex</title>
      <link>https://vaneyckt.io/posts/ruby_concurrency_in_praise_of_the_mutex/</link>
      <pubDate>Thu, 17 Mar 2016 19:12:21 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/ruby_concurrency_in_praise_of_the_mutex/</guid>
      <description>

&lt;p&gt;When reading about Ruby you will inevitably be introduced to the Global Interpreter Lock. This mechanism tends to come up in explanations of why Ruby threads run concurrently on a single core, rather than being scheduled across multiple cores in true parallel fashion. This single core scheduling approach also explains why adding threads to a Ruby program does not necessarily result in faster execution times.&lt;/p&gt;

&lt;p&gt;This post will start by explaining some of the details behind the GIL. Next up, we&amp;rsquo;ll take a look at the three crucial concepts of concurrency: atomicity, visibility, and ordering. While most developers are familiar with atomicity, the concept of visibility is often not very well understood. We will be going over these concepts in quite some detail and will illustrate how to address their needs through correct usage of the mutex data structure.&lt;/p&gt;

&lt;h3 id=&#34;parallelism-and-the-gil:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Parallelism and the GIL&lt;/h3&gt;

&lt;p&gt;Ruby&amp;rsquo;s Global Interpreter Lock is a global lock around the execution of Ruby code. Before a Ruby thread can execute any code, it first needs to acquire this lock. A thread holding the GIL will be forced to release it after a certain amount of time, at which point the kernel can hand the GIL to another Ruby thread. As the GIL can only be held by one thread at a time, it effectively prevents two Ruby threads from being executed at the same time.&lt;/p&gt;

&lt;p&gt;Luckily Ruby comes with an optimization that forces threads to let go off the GIL when they find themselves waiting on blocking IO to complete. Such threads will use the &lt;a href=&#34;http://linux.die.net/man/2/ppoll&#34;&gt;ppoll system call&lt;/a&gt; to be notified when their blocking IO has finished. Only then will they make an attempt to reacquire the GIL again. This type of behavior holds true for all blocking IO calls, as well as backtick and system calls. So even with the Global Interpreter Lock, Ruby is still able to have moments of true parallelism.&lt;/p&gt;

&lt;p&gt;Note that the GIL is specific to the default Ruby interpreter (&lt;a href=&#34;https://en.wikipedia.org/wiki/Ruby_MRI&#34;&gt;MRI&lt;/a&gt;) which relies on a global lock to protect its internals from race conditions. The GIL also makes it possible to safely interface the MRI interpreter with C libraries that may not be thread-safe themselves. Other interpreters have taken different approaches to the concept of a global lock; &lt;a href=&#34;http://rubinius.com/&#34;&gt;Rubinius&lt;/a&gt; opts for a collection of fine-grained locks instead of a single global one, whereas &lt;a href=&#34;http://jruby.org/&#34;&gt;JRuby&lt;/a&gt; does not use global locking at all.&lt;/p&gt;

&lt;h3 id=&#34;concurrency-and-the-mutex:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Concurrency and the Mutex&lt;/h3&gt;

&lt;p&gt;There are three crucial concepts to concurrency: atomicity, visibility, and ordering. We&amp;rsquo;ll be taking a look at how Ruby&amp;rsquo;s mutex data structure addresses these. It is worth pointing out that different languages tackle these concepts in different ways. As such, the mutex-centric approach described here is only guaranteed to work in Ruby.&lt;/p&gt;

&lt;h4 id=&#34;atomicity:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Atomicity&lt;/h4&gt;

&lt;p&gt;Atomicity is probably the best-known concurrency concept. A section of code is said to atomically modify the state of an object if all other threads are unable to see any of the intermediate states of the object being modified. These other threads either see the state as it was before the operation, or they see the state as it is after the operation.&lt;/p&gt;

&lt;p&gt;In the example below we have created a &lt;code&gt;counters&lt;/code&gt; array that holds ten entries, each of which is set to zero. This array represents an object that we want to modify, and its entries represent its internal state. Let&amp;rsquo;s say we have five threads, each of which executes a loop for 100.000 iterations that increments every entry by one. Intuitively we&amp;rsquo;d expect the output of this to be an array with each entry set to 500.000. However, as we can see below, this is not the case.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# atomicity.rb
counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

threads = 5.times.map do
  Thread.new do
    100000.times do
      counters.map! { |counter| counter + 1 }
    end
  end
end
threads.each(&amp;amp;:join)

puts counters.to_s
# =&amp;gt; [500000, 447205, 500000, 500000, 500000, 500000, 203656, 500000, 500000, 500000]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The reason for this unexpected output is that &lt;code&gt;counters.map! { |counter| counter + 1 }&lt;/code&gt; is not atomic. For example, imagine that our first thread has just read the value of the first entry, incremented it by one, and is now getting ready to write this incremented value to the first entry of our array. However, before our thread can write this incremented value, it gets interrupted by the second thread. This second thread then goes on to read the current value of the first entry, increments it by one, and succeeds in writing the result back to the first entry of our array. Now we have a problem!&lt;/p&gt;

&lt;p&gt;We have a problem because the first thread got interrupted before it had a chance to write its incremented value to the array. When the first thread resumes, it will end up overwriting the value that the second thread just placed in the array. This will cause us to essentially lose an increment operation, which explains why our program output has entries in it that are less than 500.000.&lt;/p&gt;

&lt;p&gt;It should hopefully be clear that none of this would have happened if we had made sure that &lt;code&gt;counters.map! { |counter| counter + 1 }&lt;/code&gt; was atomic. This would have made it impossible for the second thread to just come in and modify the intermediate state of the &lt;code&gt;counters&lt;/code&gt; array.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# atomicity.rb
mutex = Mutex.new
counters = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

threads = 5.times.map do
  Thread.new do
    100000.times do
      mutex.synchronize do
        counters.map! { |counter| counter + 1 }
      end
    end
  end
end
threads.each(&amp;amp;:join)

puts counters.to_s
# =&amp;gt; [500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000, 500000]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Atomicity can be accomplished by using a mutex as a locking mechanism that ensures no two threads can simultaneously execute the same section of code. The code above shows how we can prevent a thread executing &lt;code&gt;counters.map! { |counter| counter + 1 }&lt;/code&gt; from being interrupted by other threads wanting to execute the same code. Also, be sure to note that &lt;code&gt;mutex.synchronize&lt;/code&gt; only prevents a thread from being interrupted by others wanting to execute code wrapped inside the same &lt;code&gt;mutex&lt;/code&gt; variable!&lt;/p&gt;

&lt;h4 id=&#34;visibility:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Visibility&lt;/h4&gt;

&lt;p&gt;Visibility determines when the results of the actions performed by a thread become visible to other threads. For example, when a thread wants to write an updated value to memory, that updated value may end up being put in a cache for a while until the kernel decides to flush it to main memory. Other threads that read from that memory will therefore end up with a stale value!&lt;/p&gt;

&lt;p&gt;The code below shows an example of the visibility problem. Here we have several threads flipping the boolean values in the &lt;code&gt;flags&lt;/code&gt; array over and over again. The code responsible for changing these values is wrapped inside a mutex, so we know the intermediate states of the &lt;code&gt;flags&lt;/code&gt; array won&amp;rsquo;t be visible to other threads. We would thus expect the output of this program to contain the same boolean value for every entry of this array. However, we shall soon see that this does not always hold true.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# visibility.rb
mutex = Mutex.new
flags = [false, false, false, false, false, false, false, false, false, false]

threads = 50.times.map do
  Thread.new do
    100000.times do
      puts flags.to_s
      mutex.synchronize do
        flags.map! { |f| !f }
      end
    end
  end
end
threads.each(&amp;amp;:join)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby visibility.rb &amp;gt; visibility.log
$ grep -Hnri &#39;true, false&#39; visibility.log | wc -l
    30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code will produce five million lines of output. We&amp;rsquo;ll use the &lt;code&gt;&amp;gt;&lt;/code&gt; operator to write all these lines to a file. Having done this, we can then &lt;code&gt;grep&lt;/code&gt; for inconsistencies in the output. We would expect every line of the output to contain an array with all its entries set to the same boolean value. However, it turns out that this only holds true for 99.9994% of all lines. Sometimes the flipped boolean values don&amp;rsquo;t get written to memory fast enough, causing other threads to read stale data. This is a great illustration of the visibility problem.&lt;/p&gt;

&lt;p&gt;Luckily we can solve this problem by using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Memory_barrier&#34;&gt;memory barrier&lt;/a&gt;. A memory barrier enforces an ordering constraint on memory operations thereby preventing the possibility of reading stale data. In Ruby, a mutex not only acts as an atomic lock, but also functions as a memory barrier. When wanting to read the value of a variable being modified by multiple threads, a memory barrier will effectively tell your program to wait until all in-flight memory writes are complete. In practice this means that if we use a mutex when writing to a variable, we need to use this same mutex when reading from that variable as well.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# visibility.rb
mutex = Mutex.new
flags = [false, false, false, false, false, false, false, false, false, false]

threads = 50.times.map do
  Thread.new do
    100000.times do
      mutex.synchronize do
        puts flags.to_s
      end
      mutex.synchronize do
        flags.map! { |f| !f }
      end
    end
  end
end
threads.each(&amp;amp;:join)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby visibility.rb &amp;gt; visibility.log
$ grep -Hnri &#39;true, false&#39; visibility.log | wc -l
    0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As expected, this time we found zero inconsistencies in the output data due to us using the same mutex for both reading and writing the boolean values of the &lt;code&gt;flags&lt;/code&gt; array. Do keep in mind that not all languages allow for using a mutex as a memory barrier, so be sure to check the specifics of your favorite language before going off to write concurrent code.&lt;/p&gt;

&lt;h3 id=&#34;ordering:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Ordering&lt;/h3&gt;

&lt;p&gt;As if dealing with visibility isn&amp;rsquo;t hard enough, the Ruby interpreter is also allowed to change the order of the instructions in your code in an attempt at optimization. Before I continue I should point out that there is no official specification for the Ruby language. This can make it hard to find information about topics such as this. So I&amp;rsquo;m just going to describe how I &lt;em&gt;think&lt;/em&gt; instruction reordering currently works in Ruby.&lt;/p&gt;

&lt;p&gt;Your Ruby code gets compiled to bytecode by the Ruby interpreter. The interpreter is free to reorder your code in an attempt to optimize it. This bytecode will then generate a set of CPU instructions, which &lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-order_execution&#34;&gt;the CPU is free to reorder&lt;/a&gt; as well. I wasn&amp;rsquo;t able to come up with example code that actually showcases this reordering behavior, so this next bit is going to be somewhat hand-wavy. Let&amp;rsquo;s say we were given the code shown below (&lt;a href=&#34;http://jeremymanson.blogspot.ie/2007/08/atomicity-visibility-and-ordering.html&#34;&gt;original source&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# ordering.rb
a = false
b = false
threads = []

thr1 = Thread.new do
  a = true
  b = true
end

thr2 = Thread.new do
  r1 = b # could see true
  r2 = a # could see false
  r3 = a # could see true
  puts (r1 &amp;amp;&amp;amp; !r2) &amp;amp;&amp;amp; r3 # could print true
end

thr1.join
thr2.join
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Since there are a lot of ways for instruction reordering to take place, it is not impossible for &lt;code&gt;b = true&lt;/code&gt; to be executed before &lt;code&gt;a = true&lt;/code&gt;. In theory, this could therefore allow for &lt;code&gt;thr2&lt;/code&gt; to end up outputting &lt;code&gt;true&lt;/code&gt;. This is rather counterintuitive, as this would only be possible if the variable &lt;code&gt;b&lt;/code&gt; had changed value before the variable &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Luckily there is no need to worry too much about this. When looking at the code above, it should be obvious that code reordering is going to be the least of its problems. The lack of any kind of synchronization to help deal with atomicity and visibility issues in this threaded program is going to cause way bigger headaches than code reordering ever could.&lt;/p&gt;

&lt;p&gt;Those synchronization issues can be fixed by using a mutex. By introducing a mutex we are explicitly telling the interpreter and CPU how our code should behave, thus preventing any problematic code reordering from occurring. Dealing with atomicity and visibility issues will therefore implicitly prevent any dangerous code reordering.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:ea5ce71b092b908f8e251c60020af72d&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I hope this post has helped show just how easy it can be to introduce bugs in concurrent code. In my experience, the concept of memory barriers is often poorly understood, which can result in introducing some incredibly hard to find bugs. Luckily, as we saw in this post, the mutex data structure can be a veritable panacea for addressing these issues in Ruby.&lt;/p&gt;

&lt;p&gt;Please feel free to contact me if you think I got anything wrong. While all of the above is correct to the best of my knowledge, the lack of an official Ruby specification can make it hard to locate information that is definitively without error.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to write your own rspec retry mechanism</title>
      <link>https://vaneyckt.io/posts/how_to_write_your_own_rspec_retry_mechanism/</link>
      <pubDate>Sun, 17 Jan 2016 20:17:35 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/how_to_write_your_own_rspec_retry_mechanism/</guid>
      <description>

&lt;p&gt;Imagine you have an rspec test suite filled with &lt;a href=&#34;http://david.heinemeierhansson.com/2014/tdd-is-dead-long-live-testing.html&#34;&gt;system tests&lt;/a&gt;. Each system test simulates how a real user would interact with your app by opening a browser session through which it fills out text fields, clicks on buttons, and sends data to public endpoints. Unfortunately, browser drivers are not without bugs and sometimes your tests will fail because of these. Wouldn&amp;rsquo;t it be nice if we could automatically retry these failed tests?&lt;/p&gt;

&lt;p&gt;This article starts by investigating how rspec formatters can be used to help us keep track of failed tests. Next, we&amp;rsquo;ll use this information to take a first stab at creating a rake task that can automatically retry failed tests. Lastly, we&amp;rsquo;ll explore how to further improve our simple rake task so as to make it ready for use in production.&lt;/p&gt;

&lt;p&gt;Note that any code shown in this post is only guaranteed to work with rspec 3.3. In the past I&amp;rsquo;ve written similar code for other rspec versions as well though. So don&amp;rsquo;t worry, it shouldn&amp;rsquo;t be too hard to get all of this to work on whatever rspec version you find yourself using.&lt;/p&gt;

&lt;h3 id=&#34;rspec-formatters:827d6d4c89344577730dfb054dfd5b79&#34;&gt;Rspec formatters&lt;/h3&gt;

&lt;p&gt;Rspec generates its command line output by relying on formatters that receive messages on events like &lt;code&gt;example_passed&lt;/code&gt; and &lt;code&gt;example_failed&lt;/code&gt;. We can use these hooks to help us keep track of failed tests by having them write the descriptions of failed tests to a text file named &lt;code&gt;tests_failed&lt;/code&gt;. Our &lt;code&gt;FailureFormatter&lt;/code&gt; class does just that.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# failure_formatter.rb
require &#39;rspec/core/formatters/progress_formatter&#39;

class FailureFormatter &amp;lt; RSpec::Core::Formatters::ProgressFormatter
  RSpec::Core::Formatters.register self, :example_failed

  def example_failed(notification)
    super
    File.open(&#39;tests_failed&#39;, &#39;a&#39;) do |file|
      file.puts(notification.example.full_description)
    end
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ll soon have a look at how tests behave when we try to run them with the formatter shown above. But first, let&amp;rsquo;s prepare some example tests. We&amp;rsquo;ll create two tests. One of which will always pass, and another one which will always fail.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# my_fake_tests_spec.rb
describe &#39;my fake tests&#39;, :type =&amp;gt; :feature do

  it &#39;this scenario should pass&#39; do
    expect(true).to eq true
  end

  it &#39;this scenario should fail&#39; do
    expect(false).to eq true
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Having done that, we can now run our tests with the &lt;code&gt;FailureFormatter&lt;/code&gt; we wrote earlier. As you can see below, we&amp;rsquo;ll have to pass both &lt;code&gt;--require&lt;/code&gt; and &lt;code&gt;--format&lt;/code&gt; params in order to get our formatter to work. I&amp;rsquo;m also using the &lt;code&gt;--no-fail-fast&lt;/code&gt; flag so as to prevent our test suite from exiting upon encountering its first failure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
.F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &amp;lt;top (required)&amp;gt;&#39;

Finished in 0.02272 seconds (files took 0.0965 seconds to load)
2 examples, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After running this, we should now have a &lt;code&gt;tests_failed&lt;/code&gt; file that contains a single line describing our failed test. As we can see in the snippet below, this is indeed the case.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cat tests_failed

my fake tests this scenario should fail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Take a moment to reflect on what we have just done. By writing just a few lines of code we have effectively created a logging mechanism that will help us keep track of failed tests. In the next section we will look at how we can make use of this mechanism to automatically rerun failed tests.&lt;/p&gt;

&lt;h3 id=&#34;first-pass-at-creating-the-retry-task:827d6d4c89344577730dfb054dfd5b79&#34;&gt;First pass at creating the retry task&lt;/h3&gt;

&lt;p&gt;In this section we will create a rake task that runs our rspec test suite and automatically retries any failed tests. The finished rake task is shown below. For now, have a look at this code and then we&amp;rsquo;ll go over its details in the next few paragraphs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;fileutils&#39;

task :rspec_with_retries, [:max_tries] do |_, args|
  max_tries = args[:max_tries].to_i

  # construct initial rspec command
  command = &#39;bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast&#39;

  max_tries.times do |t|
    puts &amp;quot;\n&amp;quot;
    puts &#39;##########&#39;
    puts &amp;quot;### STARTING TEST RUN #{t + 1} OUT OF A MAXIMUM OF #{max_tries}&amp;quot;
    puts &amp;quot;### executing command: #{command}&amp;quot;
    puts &#39;##########&#39;

    # delete tests_failed file left over by previous run
    FileUtils.rm(&#39;tests_failed&#39;, :force =&amp;gt; true)

    # run tests
    puts `#{command}`

    # early out
    exit 0 if $?.exitstatus.zero?
    exit 1 if (t == max_tries - 1)

    # determine which tests need to be run again
    failed_tests = []
    File.open(&#39;tests_failed&#39;, &#39;r&#39;) do |file|
      failed_tests = file.readlines.map { |line| &amp;quot;\&amp;quot;#{line.strip}\&amp;quot;&amp;quot; }
    end

    # construct command to rerun just the failed tests
    command  = [&#39;bundle exec rspec&#39;]
    command += Array.new(failed_tests.length, &#39;-e&#39;).zip(failed_tests).flatten
    command += [&#39;--require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast&#39;]
    command = command.join(&#39; &#39;)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The task executes the &lt;code&gt;bundle exec rspec&lt;/code&gt; command a &lt;code&gt;max_tries&lt;/code&gt; number of times. The first iteration runs the full rspec test suite with the &lt;code&gt;FailureFormatter&lt;/code&gt; class and writes the descriptions of failed tests to a &lt;code&gt;tests_failed&lt;/code&gt; file. Subsequent iterations read from this file and use the &lt;a href=&#34;https://relishapp.com/rspec/rspec-core/v/3-3/docs/command-line/example-option&#34;&gt;-e option&lt;/a&gt; to rerun the tests listed there.&lt;/p&gt;

&lt;p&gt;Note that these subsequent iterations use the &lt;code&gt;FailureFormatter&lt;/code&gt; as well. This means that any tests that failed during the second iteration will get written to the &lt;code&gt;tests_failed&lt;/code&gt; file to be retried by the third iteration. This continues until we reach the max number of iterations or until one of our iterations has all its tests pass.&lt;/p&gt;

&lt;p&gt;Every iteration deletes the &lt;code&gt;tests_failed&lt;/code&gt; file from the previous iteration. For this we use the &lt;code&gt;FileUtils.rm&lt;/code&gt; method with the &lt;code&gt;:force&lt;/code&gt; flag set to &lt;code&gt;true&lt;/code&gt;. This flag ensures that the program doesn&amp;rsquo;t crash in case the &lt;code&gt;tests_failed&lt;/code&gt; file doesn&amp;rsquo;t exist. The above code relies on backticks to execute the &lt;code&gt;bundle exec rspec&lt;/code&gt; subprocess. Because of this we need to use the global variable &lt;code&gt;$?&lt;/code&gt; to access the exit status of this subprocess.&lt;/p&gt;

&lt;p&gt;Below you can see the output of a run of our rake task. Notice how the first iteration runs both of our tests, whereas the second and third iterations rerun just the failed test. This shows our retry mechanism is indeed working as expected.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ rake rspec_with_retries[3]

##########
### STARTING TEST RUN 1 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
.F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &amp;lt;top (required)&amp;gt;&#39;

Finished in 0.02272 seconds (files took 0.0965 seconds to load)
2 examples, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail


##########
### STARTING TEST RUN 2 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec -e &amp;quot;my fake tests this scenario should fail&amp;quot; --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
Run options: include {:full_description=&amp;gt;/my\ fake\ tests\ this\ scenario\ should\ fail/}
F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &amp;lt;top (required)&amp;gt;&#39;

Finished in 0.02286 seconds (files took 0.09094 seconds to load)
1 example, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail


##########
### STARTING TEST RUN 3 OUT OF A MAXIMUM OF 3
### executing command: bundle exec rspec -e &amp;quot;my fake tests this scenario should fail&amp;quot; --require ./spec/formatters/failure_formatter.rb --format FailureFormatter --no-fail-fast
##########
Run options: include {:full_description=&amp;gt;/my\ fake\ tests\ this\ scenario\ should\ fail/}
F

Failures:

  1) my fake tests this scenario should fail
     Failure/Error: expect(false).to eq true

       expected: true
            got: false

       (compared using ==)
     # ./spec/my_fake_tests_spec.rb:8:in `block (2 levels) in &amp;lt;top (required)&amp;gt;&#39;

Finished in 0.02378 seconds (files took 0.09512 seconds to load)
1 example, 1 failure

Failed examples:

rspec ./spec/my_fake_tests_spec.rb:7 # my fake tests this scenario should fail
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The goal of this section was to introduce the general idea behind our retry mechanism. There are however several shortcomings in the code that we&amp;rsquo;ve shown here. The next section will focus on identifying and fixing these.&lt;/p&gt;

&lt;h3 id=&#34;perfecting-the-retry-task:827d6d4c89344577730dfb054dfd5b79&#34;&gt;Perfecting the retry task&lt;/h3&gt;

&lt;p&gt;The code in the previous section isn&amp;rsquo;t all that bad, but there are a few things related to the &lt;code&gt;bundle exec rspec&lt;/code&gt; subprocess that we can improve upon. In particular, using backticks to initiate subprocesses has several downsides:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the standard output stream of the subprocess gets written into a buffer which we cannot print until the subprocess finishes&lt;/li&gt;
&lt;li&gt;the standard error stream does not even get written to this buffer&lt;/li&gt;
&lt;li&gt;the backticks approach does not return the id of the subprocess to us&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This last downside is especially bad as not having the subprocess id makes it hard for us to cancel the subprocess in case the rake task gets terminated. This is why I prefer to use the &lt;a href=&#34;https://github.com/jarib/childprocess&#34;&gt;childprocess gem&lt;/a&gt; for handling subprocesses instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;fileutils&#39;
require &#39;childprocess&#39;

task :rspec_with_retries, [:max_tries] do |_, args|
  max_tries = args[:max_tries].to_i

  # exit hook to ensure rspec process gets stopped when CTRL+C (SIGTERM is pressed)
  # needs to be set outside the times loop as otherwise each iteration would add its
  # own at_exit hook
  process = nil
  at_exit do
    process.stop unless process.nil?
  end

  # construct initial rspec command
  command = [&#39;bundle&#39;, &#39;exec&#39;, &#39;rspec&#39;, &#39;--require&#39;, &#39;./spec/formatters/failure_formatter.rb&#39;, &#39;--format&#39;, &#39;FailureFormatter&#39;, &#39;--no-fail-fast&#39;]

  max_tries.times do |t|
    puts &amp;quot;\n&amp;quot;
    puts &#39;##########&#39;
    puts &amp;quot;### STARTING TEST RUN #{t + 1} OUT OF A MAXIMUM OF #{max_tries}&amp;quot;
    puts &amp;quot;### executing command: #{command}&amp;quot;
    puts &#39;##########&#39;

    # delete tests_failed file left over by previous run
    FileUtils.rm(&#39;tests_failed&#39;, :force =&amp;gt; true)

    # run tests in separate process
    process = ChildProcess.build(*command)
    process.io.inherit!
    process.start
    process.wait

    # early out
    exit 0 if process.exit_code.zero?
    exit 1 if (t == max_tries - 1)

    # determine which tests need to be run again
    failed_tests = []
    File.open(&#39;tests_failed&#39;, &#39;r&#39;) do |file|
      failed_tests = file.readlines.map { |line| line.strip }
    end

    # construct command to rerun just the failed tests
    command  = [&#39;bundle&#39;, &#39;exec&#39;, &#39;rspec&#39;]
    command += Array.new(failed_tests.length, &#39;-e&#39;).zip(failed_tests).flatten
    command += [&#39;--require&#39;, &#39;./spec/formatters/failure_formatter.rb&#39;, &#39;--format&#39;, &#39;FailureFormatter&#39;, &#39;--no-fail-fast&#39;]
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As we can see from the line &lt;code&gt;process = ChildProcess.build(*command)&lt;/code&gt;, this gem makes it trivial to obtain the subprocess id. This then allows us to write an &lt;code&gt;at_exit&lt;/code&gt; hook that shuts this subprocess down upon termination of our rake task. For example, using ctrl+c to cease the rake task will now cause the rspec subprocess to stop as well.&lt;/p&gt;

&lt;p&gt;This gem also makes it super easy to inherit the stdout and stderr streams from the parent process (our rake task). This means that anything that gets written to the stdout and stderr streams of the subprocess will now be written directly to the stdout and stderr streams of our rake task. Or in other words, our rspec subprocess is now able to output directly to the rake task&amp;rsquo;s terminal session. Having made these improvements, our &lt;code&gt;rspec_with_retries&lt;/code&gt; task is now ready for use in production.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:827d6d4c89344577730dfb054dfd5b79&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I hope this post helped some people out there who find themselves struggling to deal with flaky tests. Please note that a retry mechanism such as this is really only possible because of rspec&amp;rsquo;s powerful formatters. Get in touch if you have any examples of other cool things built on top of this somewhat underappreciated feature!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The disaster that is Ruby&#39;s timeout method</title>
      <link>https://vaneyckt.io/posts/the_disaster_that_is_rubys_timeout_method/</link>
      <pubDate>Sat, 19 Dec 2015 19:20:03 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/the_disaster_that_is_rubys_timeout_method/</guid>
      <description>

&lt;p&gt;On paper, &lt;a href=&#34;http://ruby-doc.org/stdlib-2.1.1/libdoc/timeout/rdoc/Timeout.html#method-c-timeout&#34;&gt;Ruby&amp;rsquo;s timeout method&lt;/a&gt; looks like an incredibly useful piece of code. Ever had a network request occasionally slow down your entire program because it just wouldn&amp;rsquo;t finish? That&amp;rsquo;s where &lt;code&gt;timeout&lt;/code&gt; comes in. It provides a hard guarantee that a block of code will be finished within a specified amount of time.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;timeout&#39;

timeout(5) do
  # block of code that should be interrupted if it takes more than 5 seconds
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s one thing the documentation doesn&amp;rsquo;t tell you though. If any of the lines in that block of code introduces side effects that rely on the execution of later lines of code to leave things in a stable state, then using the &lt;code&gt;timeout&lt;/code&gt; method is a great way to introduce instability in your program. Examples of this include pretty much any program that is not entirely without stateful information. Let&amp;rsquo;s have a closer look at this method to try and figure out what&amp;rsquo;s going on here exactly.&lt;/p&gt;

&lt;h3 id=&#34;exceptions-absolutely-anywhere:9337c8a75f66f40fb43aecac823e1f80&#34;&gt;Exceptions absolutely anywhere&lt;/h3&gt;

&lt;p&gt;The problem with &lt;code&gt;timeout&lt;/code&gt; is that it relies upon Ruby&amp;rsquo;s questionable ability to have one thread raise an exception &lt;em&gt;absolutely anywhere&lt;/em&gt; in an entirely different thread. The idea is that when you place code inside a &lt;code&gt;timeout&lt;/code&gt; block, this code gets wrapped inside a new thread that executes in the background while the main thread goes to sleep for 5 seconds. Upon waking, the main thread grabs the background thread and forcefully stops it by raising a &lt;code&gt;Timeout::Error&lt;/code&gt; exception on it (&lt;a href=&#34;https://github.com/ruby/ruby/blob/trunk/lib/timeout.rb#L72-L110&#34;&gt;actual implementation&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# raising_exceptions.rb
# threads can raise exceptions in other threads
thr = Thread.new do
  puts &#39;...initializing resource&#39;
  sleep 1

  puts &#39;...using resource&#39;
  sleep 1

  puts &#39;...cleaning resource&#39;
  sleep 1
end

sleep 1.5
thr.raise(&#39;raising an exception in the thread&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby raising_exeptions.rb

...initializing resource
...using resource
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The problem with this approach is that the main thread does not care what code the background thread is executing when it raises the exception. This means that the engineer responsible for the code that gets executed by the background thread needs to assume an exception can get thrown from &lt;em&gt;absolutely anywhere&lt;/em&gt; within her code. This is madness! No one can be expected to place exception catchers around every single block of code!&lt;/p&gt;

&lt;p&gt;The following code further illustrates the problem of being able to raise an exception &lt;em&gt;absolutely anywhere&lt;/em&gt;. Turns out that &lt;em&gt;absolutely anywhere&lt;/em&gt; includes locations like the inside of &lt;code&gt;ensure&lt;/code&gt; blocks. These locations are generally not designed for handling any exceptions at all. I hope you weren&amp;rsquo;t using an &lt;code&gt;ensure&lt;/code&gt; block to terminate your database connection!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# ensure_block.rb
# raising exceptions inside an ensure block of another thread
# note how we never finish cleaning the resource here
thr = Thread.new do
  begin
    puts &#39;...initializing resource&#39;
    sleep 1

    raise &#39;something went wrong&#39;

    puts &#39;...using resource&#39;
    sleep 1
  ensure
    puts &#39;...started cleaning resource&#39;
    sleep 1
    puts &#39;...finished cleaning resource&#39;
  end
end

sleep 1.5
thr.raise(&#39;raising an exception in the thread&#39;)

# prevent program from immediately terminating after raising exception
sleep 5
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby ensure_blocks.rb

...initializing resource
...started cleaning resource
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;real-world-example:9337c8a75f66f40fb43aecac823e1f80&#34;&gt;Real world example&lt;/h3&gt;

&lt;p&gt;Recently, I spent a lot of time working with the &lt;a href=&#34;https://github.com/taf2/curb&#34;&gt;curb http client&lt;/a&gt;. I ended up wrapping quite a few of my curb calls within &lt;code&gt;timeout&lt;/code&gt; blocks because of tight time constraints. However, this caused great instability within the system I was working on. Sometimes a call would work, whereas other times that very same call would throw an exception about an invalid handle. It was this that caused me to start investigating the &lt;code&gt;timeout&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;After having a bit of think, I came up with a proof of concept that showed beyond a doubt that the &lt;code&gt;timeout&lt;/code&gt; method was introducing instability in the very internals of my http client. The finished proof of concept code can look a bit complex, so rather than showing the final concept code straightaway, I&amp;rsquo;ll run you through my thought process instead.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with the basics and write some code that uses the http client to fetch a random google page. A randomized parameter is added to the google url in order to circumvent any client-side caching. The page fetch itself is wrapped inside a &lt;code&gt;timeout&lt;/code&gt; block as we are interested in testing whether the &lt;code&gt;timeout&lt;/code&gt; method is corrupting the http client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# basics.rb
# timeout doesn&#39;t get triggered
require &#39;curb&#39;
require &#39;timeout&#39;

timeout(1) do
  Curl.get(&amp;quot;http://www.google.com?foo=#{rand}&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code will rarely timeout as a page fetch generally takes way less than one second to complete. This is why we&amp;rsquo;re going to wrap our page fetch inside an infinite while loop.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# infinite_loop.rb
# timeout gets triggered and Timeout::Error exception gets thrown
require &#39;curb&#39;
require &#39;timeout&#39;

timeout(1) do
  while true
    Curl.get(&amp;quot;http://www.google.com?foo=#{rand}&amp;quot;)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby infinite_loop.rb

/Users/vaneyckt/.rvm/gems/ruby-2.0.0-p594/gems/curb-0.8.8/lib/curl/easy.rb:68:
  in &#39;perform&#39;: execution expired (Timeout::Error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code is now timing out and throwing a &lt;code&gt;Timeout::Error&lt;/code&gt; exception. Next we want to determine whether the timing out of a page fetch could corrupt the internal state of the http client, thereby causing problems for a subsequent page fetch. We&amp;rsquo;ll need to make lots of page fetches to test this, so we&amp;rsquo;re going to wrap all of our current code inside another infinite while loop. Furthermore, we don&amp;rsquo;t want any &lt;code&gt;Timeout::Error&lt;/code&gt; exceptions to break us out of this while loop, so we&amp;rsquo;re going to catch and ignore these exceptions inside the while loop we just created. This gives us our finished proof of concept code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# proof_of_concept.rb
# timeout corrupts the very internals of the curb http client
require &#39;curb&#39;
require &#39;timeout&#39;

while true
  begin
    timeout(1) do
      while true
        Curl.get(&amp;quot;http://www.google.com?foo=#{rand}&amp;quot;)
      end
    end
  rescue Timeout::Error =&amp;gt; e
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ruby proof_of_concept.rb

/Users/vaneyckt/.rvm/gems/ruby-2.0.0-p594/gems/curb-0.8.8/lib/curl/easy.rb:67:
  in &#39;add&#39;: CURLError: The easy handle is already added to a multi handle
  (Curl::Err::MultiAddedAlready)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the above program will result in an exception being thrown after a few seconds. At some point, the &lt;code&gt;timeout&lt;/code&gt; method is causing a &lt;code&gt;Timeout::Error&lt;/code&gt; exception to be raised inside a critical code path of the http client. This badly timed &lt;code&gt;Timeout::Error&lt;/code&gt; exception leaves the client in an invalid state, which in turn causes the next page fetch to fail with the exception shown above. Hopefully this illustrates why you should avoid creating programs that can have &lt;code&gt;Timeout::Error&lt;/code&gt; exceptions pop up &lt;em&gt;absolutely anywhere&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:9337c8a75f66f40fb43aecac823e1f80&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I hope this has convinced you there is nothing you can do to prevent &lt;code&gt;timeout&lt;/code&gt; from doing whatever it wants to your program&amp;rsquo;s internal state. There is just no way a program can deal with &lt;code&gt;Timeout::Error&lt;/code&gt; exceptions being able to potentially pop up &lt;em&gt;absolutely anywhere&lt;/em&gt;. The only time you can really get away with using timeouts is when writing functional code that does not rely on any state. In all other cases, it is best to just avoid timeouts entirely.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A javascript closures recap</title>
      <link>https://vaneyckt.io/posts/a_javascript_closures_recap/</link>
      <pubDate>Sat, 26 Sep 2015 17:54:23 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/a_javascript_closures_recap/</guid>
      <description>

&lt;p&gt;Javascript closures have always been one those things that I used to navigate by intuition. Recently however, upon stumbling across some code that I did not quite grok, it became clear I should try and obtain a more formal understanding. This post is mainly intended as a quick recap for my future self. It won&amp;rsquo;t go into all the details about closures; instead it will focus on the bits that I found most helpful.&lt;/p&gt;

&lt;p&gt;There seem to be very few step-by-step overviews of javascript closures. As a matter of fact, I only found two. Luckily they are both absolute gems. You can find them &lt;a href=&#34;http://openhome.cc/eGossip/JavaScript/Closures.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://web.archive.org/web/20080209105120/http://blog.morrisjohns.com/javascript_closures_for_dummies&#34;&gt;here&lt;/a&gt;. I heartily recommend both these articles to anyone wanting to gain a more complete understanding of closures.&lt;/p&gt;

&lt;h3 id=&#34;closure-basics:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Closure basics&lt;/h3&gt;

&lt;p&gt;I&amp;rsquo;m going to shamelessly borrow a few lines from the &lt;a href=&#34;http://openhome.cc/eGossip/JavaScript/Closures.html&#34;&gt;first&lt;/a&gt; of the two articles linked above to illustrate the basic concept of a closure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;function doSome() {
  var x = 10;

  function f(y) {
    return x + y;
  }
  return f;
}

var foo = doSome();
foo(20); // returns 30
foo(30); // returns 40
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;In the above example, the function f creates a closure. If you just look at f, it seems that the variable x is not defined. Actually, x is caught from the enclosing function. A closure is a function which closes (or survives) variables of the enclosing function. In the above example, the function f creates a closure because it closes the variable x into the scope of itself. If the closure object, a Function instance, is still alive, the closed variable x keeps alive. It&amp;rsquo;s like that the scope of the variable x is extended.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is really all you need to know about closures: they refer to variables declared outside the scope of the function and by doing so keep these variables alive. Closure behavior can be entirely explained just by keeping these two things in mind.&lt;/p&gt;

&lt;h3 id=&#34;closures-and-primitive-data-types:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Closures and primitive data types&lt;/h3&gt;

&lt;p&gt;The rest of this post will go over some code examples to illustrate the behavior of closures for both primitive and object params. In this section, we&amp;rsquo;ll have a look at the behavior of a closure with a primitive data type param.&lt;/p&gt;

&lt;h4 id=&#34;example-1:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 1&lt;/h4&gt;

&lt;p&gt;The code below will be our starting point for studying closures. Be sure to take a good look at it, as all our examples will be a variation of this code. Throughout this post, we are going to try and understand closures by examining the values returned by the &lt;code&gt;foo()&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var prim = 1;

var foo = function(p) {
  var f = function() {
    return p;
  }
  return f;
}(prim);

foo();    // returns 1
prim = 3;
foo();    // returns 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the javascript runtime wants to resolve the value returned by &lt;code&gt;return p;&lt;/code&gt;, it finds that this p variable is the same as the p variable from &lt;code&gt;var foo = function(p) {&lt;/code&gt;. In other words, there is no direct link between the p from &lt;code&gt;return p;&lt;/code&gt; and the variable prim from &lt;code&gt;var prim = 1;&lt;/code&gt;. We see this is true because assigning a new value to prim does not cause the value returned by &lt;code&gt;foo()&lt;/code&gt; to change.&lt;/p&gt;

&lt;h4 id=&#34;example-2:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 2&lt;/h4&gt;

&lt;p&gt;Now let&amp;rsquo;s have a look at what happens when we make a small change to the previous code sample by adding the line &lt;code&gt;p = 2;&lt;/code&gt; to it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var prim = 1;

var foo = function(p) {
  var f = function() {
    return p;
  }
  p = 2;
  return f;
}(prim);

foo();    // returns 2
prim = 3;
foo();    // returns 2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code above is interesting in that it shows that the p variable from &lt;code&gt;return p;&lt;/code&gt; is indeed the same as the p variable from &lt;code&gt;var foo = function(p) {&lt;/code&gt;. Even though the variable f gets created at a time when p is set to 1, the act of setting p to 2 does indeed cause the value returned by &lt;code&gt;foo()&lt;/code&gt; to change. This is a great example of a closure keeping a closed variable alive.&lt;/p&gt;

&lt;h4 id=&#34;example-3:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 3&lt;/h4&gt;

&lt;p&gt;This sample shows code similar to the first, but this time we made the closure close over the prim variable.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var prim = 1;

var foo = function() {
  return prim;
}

foo();    // returns 1
prim = 3;
foo();    // returns 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here too we can make a similar deduction as we did for the previous samples. When the javascript runtime wants to resolve the value returned by &lt;code&gt;return prim;&lt;/code&gt;, it finds that this prim variable is the same as the prim variable from &lt;code&gt;var prim = 1;&lt;/code&gt;. This explains why setting prim to 3 causes the value returned by &lt;code&gt;foo()&lt;/code&gt; to change.&lt;/p&gt;

&lt;h3 id=&#34;closures-and-objects:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Closures and objects&lt;/h3&gt;

&lt;p&gt;In this section we&amp;rsquo;ll see what happens when we take our code samples and change the param from a primitive data type to an object.&lt;/p&gt;

&lt;h4 id=&#34;example-1-a:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 1.a&lt;/h4&gt;

&lt;p&gt;The code below is interesting because in the previous section we saw that a similar example using a primitive param had both calls to &lt;code&gt;foo()&lt;/code&gt; return the same value. So what&amp;rsquo;s different here? Let&amp;rsquo;s inspect how the runtime resolves the variables involved.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var obj = [&amp;quot;a&amp;quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  return f;
}(obj);

foo();        // returns 1
obj[1] = &amp;quot;b&amp;quot;; // modifies the object pointed to by the obj var
obj[2] = &amp;quot;c&amp;quot;; // modifies the object pointed to by the obj var
foo();        // returns 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When the runtime tries to resolve the variable o from &lt;code&gt;return o.length;&lt;/code&gt;, it finds that this variable o is the same as the variable o from &lt;code&gt;var foo = function(o) {&lt;/code&gt;. We saw this exact same thing in the previous section. Unlike the previous section, the variable o now contains a reference to an array object. This causes our closure to have a direct link to this array object, and thus any changes to it will get reflected in the output of &lt;code&gt;foo()&lt;/code&gt;. This explains why the second call to &lt;code&gt;foo()&lt;/code&gt; gives a different output than the first.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A good rule of thumb goes like this:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;if a closed variable contains a value, then the closure links to that variable&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;if a closed variable contains a reference to an object, then the closure links to that object, and will pick up on any changes made to it&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;example-1-b:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 1.b&lt;/h4&gt;

&lt;p&gt;Note that the closure will only pick up on changes made to the particular object that was present when the closure was created. Assigning a new object to the obj variable after the closure was created will have no effect. The code below illustrates this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var obj = [&amp;quot;a&amp;quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  return f;
}(obj);

foo();                 // returns 1
obj = [&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;]; // assign a new array object to the obj variable
foo();                 // returns 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In fact, this code is practically identical to the code from Example 1 of the previous section.&lt;/p&gt;

&lt;h4 id=&#34;example-2-1:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 2&lt;/h4&gt;

&lt;p&gt;We&amp;rsquo;ll now modify the previous code sample a bit. This time we&amp;rsquo;ll take a look at what happens when we add the line &lt;code&gt;o[1] = &amp;quot;b&amp;quot;;&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var obj = [&amp;quot;a&amp;quot;];

var foo = function(o) {
  var f = function() {
    return o.length;
  }
  o[1] = &amp;quot;b&amp;quot;;
  return f;
}(obj);

foo();        // returns 2
obj[1] = &amp;quot;b&amp;quot;;
obj[2] = &amp;quot;c&amp;quot;;
foo();        // returns 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once again, we can start by reasoning about how the runtime resolves the variable o from &lt;code&gt;return o.length;&lt;/code&gt;. As you probably know by now, this variable o is the same as the variable o from &lt;code&gt;var foo = function(o) {&lt;/code&gt;. And since it contains a reference to an object, any changes to this object will get reflected in the output of &lt;code&gt;foo()&lt;/code&gt;. This explains why the first call to &lt;code&gt;foo()&lt;/code&gt; now returns 2, whereas previously it was returning 1.&lt;/p&gt;

&lt;h4 id=&#34;example-3-1:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Example 3&lt;/h4&gt;

&lt;p&gt;If you managed to make it this far, this last bit of code should hold no surprises for you.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var obj = [&amp;quot;a&amp;quot;];

var foo = function() {
  return obj.length;
}

foo();        // returns 1
obj[1] = &amp;quot;b&amp;quot;;
obj[2] = &amp;quot;c&amp;quot;;
foo();        // returns 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The runtime will resolve the variable obj from &lt;code&gt;return obj.length;&lt;/code&gt; to be the same as the variable obj from &lt;code&gt;var obj = [&amp;quot;a&amp;quot;];&lt;/code&gt;. As a result, any changes to the obj variable will have an effect on the output of &lt;code&gt;foo()&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:fd3eb7d414a05af8a77db8210b1ff256&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Hopefully this post has demystified closures a bit. Time and time again, we&amp;rsquo;ve shown how following a few simple steps will lead you to understand their behavior. Just keep in mind these rules of thumb and you should be good to go:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if a closed variable contains a value, then the closure links to that variable&lt;/li&gt;
&lt;li&gt;if a closed variable contains a reference to an object, then the closure links to that object, and will pick up on any changes made to it&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally, this is going to become my go-to post for providing an introduction to closures. So please let me know any suggestions you might have to improve this post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding iostat</title>
      <link>https://vaneyckt.io/posts/understanding_iostat/</link>
      <pubDate>Mon, 24 Aug 2015 19:47:55 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/understanding_iostat/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been spending a lot of time lately looking at I/O performance and reading up about the &lt;code&gt;iostat&lt;/code&gt; command. While this command provides a wealth of I/O performance data, the sheer amount of it all can make it hard to see the forest for the trees. In this post, we&amp;rsquo;ll talk about interpreting this data. Before we continue, I would first like to thank the authors of the blog posts mentioned below, as each of these has helped me understand &lt;code&gt;iostat&lt;/code&gt; and its many complexities just a little bit better.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.thattommyhall.com/2011/02/18/iops-linux-iostat/&#34;&gt;Measuring disk usage in linux (%iowait vs IOPS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pythian.com/blog/basic-io-monitoring-on-linux/&#34;&gt;Basic I/O monitoring on linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://brooker.co.za/blog/2014/07/04/iostat-pct.html&#34;&gt;Two traps in iostat: %util and svctm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.pregos.info/wp-content/uploads/2010/09/iowait.txt&#34;&gt;What exactly is &amp;ldquo;iowait&amp;rdquo;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.igvita.com/2009/06/23/measuring-optimizing-io-performance/&#34;&gt;Measuring &amp;amp; optimizing I/O performance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://dom.as/2009/03/11/iostat/&#34;&gt;Iostat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.xaprb.com/blog/2010/09/06/beware-of-svctm-in-linuxs-iostat/&#34;&gt;Beware of svctm in linux&amp;rsquo;s iostat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.psce.com/blog/2012/04/18/analyzing-io-performance/&#34;&gt;Analyzing I/O performance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;iostat&lt;/code&gt; command can display both basic and extended metrics. We&amp;rsquo;ll take a look at the basic metrics first before moving on to extended metrics in the remainder of this post. Note that this post will not go into detail about every last metric. Instead, I have decided to focus on just those metrics that I found to be especially useful, as well as those that seem to be often misunderstood.&lt;/p&gt;

&lt;h3 id=&#34;basic-iostat-metrics:e3bfd6583e1363a6dd96b0cce07a2f7c&#34;&gt;Basic iostat metrics&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;iostat&lt;/code&gt; command lists basic metrics by default. The &lt;code&gt;-m&lt;/code&gt; parameter causes metrics to be displayed in megabytes per second instead of blocks or kilobytes per second. Using the &lt;code&gt;5&lt;/code&gt; parameter causes &lt;code&gt;iostat&lt;/code&gt; to recalculate metrics every 5 seconds, thereby making the numbers an average over this interval.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ iostat -m 5

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           8.84    0.16    3.91    7.73    0.04   79.33

Device:            tps    MB_read/s    MB_wrtn/s    MB_read    MB_wrtn
xvdap1           46.34         0.33         1.03    2697023    8471177
xvdb              0.39         0.00         0.01       9496      71349
xvdg             65.98         1.34         0.97   11088426    8010609
xvdf            205.17         1.62         2.68   13341297   22076001
xvdh             51.16         0.64         1.43    5301463   11806257
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;tps&lt;/code&gt; number here is the number of I/O Operations Per Second (IOPS). Wikipedia has &lt;a href=&#34;https://en.wikipedia.org/wiki/IOPS#Examples&#34;&gt;a nice list of average IOPS for different storage devices&lt;/a&gt;. This should give you a pretty good idea of the I/O load on your machine.&lt;/p&gt;

&lt;p&gt;Some people put a lot of faith in the &lt;code&gt;%iowait&lt;/code&gt; metric as an indicator of I/O performance. However, &lt;code&gt;%iowait&lt;/code&gt; is first and foremost a CPU metric that measures the percentage of time the CPU is idle while waiting for an I/O operation to complete. This metric is heavily influenced by both your CPU speed and CPU load and is therefore easily misinterpreted.&lt;/p&gt;

&lt;p&gt;For example, consider a system with just two processes: the first one heavily I/O intensive, the second one heavily CPU intensive. As the second process will prevent the CPU from going idle, the &lt;code&gt;%iowait&lt;/code&gt; metric will stay low despite the first process&amp;rsquo;s high I/O utilization. Other examples illustrating the deceptive nature of &lt;code&gt;%iowait&lt;/code&gt; can be found &lt;a href=&#34;https://blog.pregos.info/wp-content/uploads/2010/09/iowait.txt&#34;&gt;here&lt;/a&gt; (&lt;a href=&#34;https://gist.github.com/vaneyckt/58028fb0ddbdbf561e60&#34;&gt;mirror&lt;/a&gt;). The only thing &lt;code&gt;%iowait&lt;/code&gt; really tells us is that the CPU occasionally idles while there is an outstanding I/O request, and could thus be made to handle more computational work.&lt;/p&gt;

&lt;h3 id=&#34;extended-iostat-metrics:e3bfd6583e1363a6dd96b0cce07a2f7c&#34;&gt;Extended iostat metrics&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s now take a look at the extended metrics by calling the &lt;code&gt;iostat -x&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ iostat -mx 5

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           8.84    0.16    3.91    7.73    0.04   79.33

Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
xvdap1            0.57     6.38   20.85   25.49     0.33     1.03    59.86     0.27   17.06   13.15   20.25   1.15   5.33
xvdb              0.00     1.93    0.10    0.29     0.00     0.01    51.06     0.00    7.17    0.33    9.66   0.09   0.00
xvdg              0.55     4.69   42.04   23.94     1.34     0.97    71.89     0.44    6.63    6.82    6.28   1.16   7.67
xvdf              7.33    41.35  132.66   72.52     1.62     2.68    42.87     0.49    2.37    2.79    1.59   0.36   7.42
xvdh              0.00     4.54   15.54   35.63     0.64     1.43    83.04     0.00   10.22    8.39   11.02   1.30   6.68
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;r/s&lt;/code&gt; and &lt;code&gt;w/s&lt;/code&gt; numbers show the amount of read and write requests issued to the I/O device per second. These numbers provide a more detailed breakdown of the &lt;code&gt;tps&lt;/code&gt; metric we saw earlier, as &lt;code&gt;tps = r/s + w/s&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;avgqu-sz&lt;/code&gt; metric is an important value. Its name is rather poorly chosen as it doesn&amp;rsquo;t actually show the number of operations that are queued but not yet serviced. Instead, it shows &lt;a href=&#34;http://www.xaprb.com/blog/2010/01/09/how-linux-iostat-computes-its-results&#34;&gt;the number of operations that were either queued, or being serviced&lt;/a&gt;. Ideally, you&amp;rsquo;d want to have an idea of this value during normal operations for use as a baseline number for when trouble occurs. Single digit numbers with the occasional double digit spike are safe(ish) values. Triple digit numbers generally are not.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;await&lt;/code&gt; metric is the average time from when a request was put in the queue to when the request was completed. This is the sum of the time a request was waiting in the queue and the time our storage device was working on servicing the request. This metric is highly dependent on the number of items in the queue. Much like &lt;code&gt;avgqu-sz&lt;/code&gt;, you&amp;rsquo;ll want to have an idea of the value of this metric during normal operations for use as a baseline.&lt;/p&gt;

&lt;p&gt;Our next metric is &lt;code&gt;svctm&lt;/code&gt;. You&amp;rsquo;ll find a lot of older blog posts that go into quite some detail about this one. However, &lt;code&gt;man iostat&lt;/code&gt; makes it quite clear that this metric has since been deprecated and should no longer be trusted.&lt;/p&gt;

&lt;p&gt;Our last metric is &lt;code&gt;%util&lt;/code&gt;. Just like &lt;code&gt;svctm&lt;/code&gt;, this metric has been touched by the progress of technology as well. The &lt;code&gt;man iostat&lt;/code&gt; pages contain the information shown below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;%util&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Percentage of elapsed time during which I/O requests were issued to the device (bandwidth utilization for the device). Device saturation occurs when this value is close to 100% for devices serving requests serially. But for devices serving requests in parallel, such as RAID arrays and modern SSDs, this number does not reflect their performance limits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It’s common to assume that the closer a device gets to 100% utilization, the more saturated it becomes. This is true when the storage device corresponds to a single magnetic disk as such a device can only serve one request at a time. However, a single SSD or a RAID array consisting of multiple disks can serve multiple requests simultaneously. For such devices, &lt;code&gt;%util&lt;/code&gt; essentially indicates the percentage of time that the device was busy serving one or more requests. Unfortunately, this value tells us absolutely nothing about the maximum number of simultaneous requests such a device can handle. This metric should therefore not be treated as a saturation indicator for either SSDs or RAID arrays.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:e3bfd6583e1363a6dd96b0cce07a2f7c&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;By now it should be clear that &lt;code&gt;iostat&lt;/code&gt; is an incredibly powerful tool, the metrics of which can take some experience to interpret correctly. In a perfect world your machines should regularly be writing these metrics to a monitoring service, so you&amp;rsquo;ll always have access to good baseline numbers. In an imperfect world, just knowing your baseline IOPS values will already go a long way when trying to diagnose whether a slowdown is I/O related.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Safer bash scripts with &#39;set -euxo pipefail&#39;</title>
      <link>https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/</link>
      <pubDate>Mon, 16 Mar 2015 19:43:34 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/</guid>
      <description>

&lt;p&gt;Often times developers go about writing bash scripts the same as writing code in a higher-level language. This is a big mistake as higher-level languages offer safeguards that are not present in bash scripts by default. For example, a Ruby script will throw an error when trying to read from an uninitialized variable, whereas a bash script won&amp;rsquo;t. In this article, we&amp;rsquo;ll look at how we can improve on this.&lt;/p&gt;

&lt;p&gt;The bash shell comes with several builtin commands for modifying the behavior of the shell itself. We are particularly interested in the &lt;a href=&#34;https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html&#34;&gt;set builtin&lt;/a&gt;, as this command has several options that will help us write safer scripts. I hope to convince you that it&amp;rsquo;s a really good idea to add &lt;code&gt;set -euxo pipefail&lt;/code&gt; to the beginning of all your future bash scripts.&lt;/p&gt;

&lt;h3 id=&#34;set-e:d0406b09675b080255aad6f1a20a9332&#34;&gt;set -e&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;-e&lt;/code&gt; option will cause a bash script to exit immediately when a command fails. This is generally a vast improvement upon the default behavior where the script just ignores the failing command and continues with the next line. This option is also smart enough to not react on failing commands that are part of conditional statements. Moreover, you can append a command with &lt;code&gt;|| true&lt;/code&gt; for those rare cases where you don&amp;rsquo;t want a failing command to trigger an immediate exit.&lt;/p&gt;

&lt;h4 id=&#34;before:d0406b09675b080255aad6f1a20a9332&#34;&gt;Before&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

# &#39;foo&#39; is a non-existing command
foo
echo &amp;quot;bar&amp;quot;

# output
# ------
# line 4: foo: command not found
# bar
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;after:d0406b09675b080255aad6f1a20a9332&#34;&gt;After&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -e

# &#39;foo&#39; is a non-existing command
foo
echo &amp;quot;bar&amp;quot;

# output
# ------
# line 5: foo: command not found
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;prevent-immediate-exit:d0406b09675b080255aad6f1a20a9332&#34;&gt;Prevent immediate exit&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -e

# &#39;foo&#39; is a non-existing command
foo || true
echo &amp;quot;bar&amp;quot;

# output
# ------
# line 5: foo: command not found
# bar
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;set-o-pipefail:d0406b09675b080255aad6f1a20a9332&#34;&gt;set -o pipefail&lt;/h3&gt;

&lt;p&gt;The bash shell normally only looks at the exit code of the last command of a pipeline. This behavior is not ideal as it causes the &lt;code&gt;-e&lt;/code&gt; option to only be able to act on the exit code of a pipeline&amp;rsquo;s last command. This is where &lt;code&gt;-o pipefail&lt;/code&gt; comes in. This particular option sets the exit code of a pipeline to that of the rightmost command to exit with a non-zero status, or zero if all commands of the pipeline exit successfully.&lt;/p&gt;

&lt;h4 id=&#34;before-1:d0406b09675b080255aad6f1a20a9332&#34;&gt;Before&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -e

# &#39;foo&#39; is a non-existing command
foo | echo &amp;quot;a&amp;quot;
echo &amp;quot;bar&amp;quot;

# output
# ------
# a
# line 5: foo: command not found
# bar
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;after-1:d0406b09675b080255aad6f1a20a9332&#34;&gt;After&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -eo pipefail

# &#39;foo&#39; is a non-existing command
foo | echo &amp;quot;a&amp;quot;
echo &amp;quot;bar&amp;quot;

# output
# ------
# a
# line 5: foo: command not found
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;set-u:d0406b09675b080255aad6f1a20a9332&#34;&gt;set -u&lt;/h3&gt;

&lt;p&gt;This option causes the bash shell to treat unset variables as an error and exit immediately. This brings us much closer to the behavior of higher-level languages.&lt;/p&gt;

&lt;h4 id=&#34;before-2:d0406b09675b080255aad6f1a20a9332&#34;&gt;Before&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -eo pipefail

echo $a
echo &amp;quot;bar&amp;quot;

# output
# ------
#
# bar
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;after-2:d0406b09675b080255aad6f1a20a9332&#34;&gt;After&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -euo pipefail

echo $a
echo &amp;quot;bar&amp;quot;

# output
# ------
# line 5: a: unbound variable
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;set-x:d0406b09675b080255aad6f1a20a9332&#34;&gt;set -x&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;-x&lt;/code&gt; option causes bash to print each command before executing it. This can be of great help when you have to try and debug a bash script failure through its logs. Note that arguments get expanded before a command gets printed. This causes our logs to display the actual argument values at the time of execution!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash
set -euxo pipefail

a=5
echo $a
echo &amp;quot;bar&amp;quot;

# output
# ------
# + a=5
# + echo 5
# 5
# + echo bar
# bar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. I hope this post showed you why using &lt;code&gt;set -euxo pipefail&lt;/code&gt; is such a good idea. If you have any other options you want to suggest, then please let me know and I&amp;rsquo;ll be happy to add them to this list.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to javascript promises</title>
      <link>https://vaneyckt.io/posts/an_introduction_to_javascript_promises/</link>
      <pubDate>Sat, 07 Feb 2015 18:34:09 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/an_introduction_to_javascript_promises/</guid>
      <description>

&lt;p&gt;I recently had to write some javascript code that required the sequential execution of half a dozen asynchronous requests. I figured this was the perfect time to learn a bit more about javascript promises. This post is a recap of what I read in these &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/es6/promises/&#34;&gt;three&lt;/a&gt; &lt;a href=&#34;http://www.mullie.eu/how-javascript-promises-work/&#34;&gt;amazing&lt;/a&gt; &lt;a href=&#34;http://www.sitepoint.com/overview-javascript-promises/&#34;&gt;write-ups&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;what-are-promises:e45390df332d7d109b53a5ac1df46eec&#34;&gt;What are promises?&lt;/h3&gt;

&lt;p&gt;A Promise object represents a value that may not be available yet, but will be resolved at some point in future. This abstraction allows you to write asynchronous code in a more synchronous fashion. For example, you can use a Promise object to represent data that will eventually be returned by a call to a remote web service. The &lt;code&gt;then&lt;/code&gt; and &lt;code&gt;catch&lt;/code&gt; methods can be used to attach callbacks that will be triggered once the data arrives. We&amp;rsquo;ll take a closer look at these two methods in the next sections. For now, let&amp;rsquo;s write a simple AJAX request example that prints a random joke.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &amp;quot;http://api.icndb.com/jokes/random&amp;quot;,
    success: function(result) {
      resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
    }
  });
});

promise.then(function(result) {
  console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note how the Promise object is just a wrapper around the AJAX request and how we&amp;rsquo;ve instructed the &lt;code&gt;success&lt;/code&gt; callback to trigger the &lt;code&gt;resolve&lt;/code&gt; method. We&amp;rsquo;ve also attached a callback to our Promise object with the &lt;code&gt;then&lt;/code&gt; method. This callback gets triggered when the &lt;code&gt;resolve&lt;/code&gt; method gets called. The &lt;code&gt;result&lt;/code&gt; variable of this callback will contain the data that was passed to the &lt;code&gt;resolve&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;Before we take a closer look at the &lt;code&gt;resolve&lt;/code&gt; method, let&amp;rsquo;s first investigate the Promise object a bit more. A Promise object can have one of three states:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;fulfilled&lt;/strong&gt; - the action relating to the Promise succeeded&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;rejected&lt;/strong&gt; - the action relating to the Promise failed&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pending&lt;/strong&gt; - the Promise hasn&amp;rsquo;t been fulfilled or rejected yet&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A pending Promise object can be fulfilled or rejected by calling &lt;code&gt;resolve&lt;/code&gt; or &lt;code&gt;reject&lt;/code&gt; on it. Once a Promise is fulfilled or rejected, this state gets permanently associated with it. The state of a fulfilled Promise also includes the data that was passed to &lt;code&gt;resolve&lt;/code&gt;, just as the state of a rejected Promise also includes the data that was passed to &lt;code&gt;reject&lt;/code&gt;. In summary, we can say that a Promise executes only once and stores the result of its execution.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &amp;quot;http://api.icndb.com/jokes/random&amp;quot;,
    success: function(result) {
      resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
    }
  });
});

promise.then(function(result) {
  console.log(result);
});

promise.then(function(result) {
  console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can test whether a Promise only ever executes once by adding a second callback to the previous example. In this case, we see that only one AJAX request gets made and that the same joke gets printed to the console twice. This clearly shows that our Promise was only executed once.&lt;/p&gt;

&lt;h3 id=&#34;the-then-method-and-chaining:e45390df332d7d109b53a5ac1df46eec&#34;&gt;The &lt;code&gt;then&lt;/code&gt; method and chaining&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;then&lt;/code&gt; method takes two arguments: a mandatory success callback and an optional failure callback. These callbacks are called when the Promise is settled (i.e. either fulfilled or rejected). If the Promise was fulfilled, the success callback will be fired with the data you passed to &lt;code&gt;resolve&lt;/code&gt;. If the Promise was rejected, the failure callback will be called with the data you passed to &lt;code&gt;reject&lt;/code&gt;. We&amp;rsquo;ve already covered most of this in the previous section.&lt;/p&gt;

&lt;p&gt;The real magic with the &lt;code&gt;then&lt;/code&gt; method happens when you start chaining several of them together. This chaining allows you to express your logic in separate stages, each of which can be made responsible for transforming data passed on by the previous stage or for running additional asynchronous requests. The code below shows how data returned by the success callback of the first &lt;code&gt;then&lt;/code&gt; method becomes available to the success callback of the second &lt;code&gt;then&lt;/code&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &amp;quot;http://api.icndb.com/jokes/random&amp;quot;,
    success: function(result) {
      resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
    }
  });
});

promise.then(function(result) {
  return result;
}).then(function(result) {
  console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This chaining is possible because the &lt;code&gt;then&lt;/code&gt; method returns a new Promise object that will resolve to the return value of the callback. Or in other words, by calling &lt;code&gt;return result;&lt;/code&gt; we cause the creation of an anonymous Promise object that looks something like shown below. Notice that this particular anonymous Promise object will resolve immediately, as it does not make any asynchronous requests.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;new Promise(function(resolve, reject) {
  resolve(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we understand that the &lt;code&gt;then&lt;/code&gt; method always returns a Promise object, let&amp;rsquo;s take a look at what happens when we tell the callback of a &lt;code&gt;then&lt;/code&gt; method to explicitly return a Promise object.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;function getJokePromise() {
  return new Promise(function(resolve, reject) {
    $.ajax({
      url: &amp;quot;http://api.icndb.com/jokes/random&amp;quot;,
      success: function(result) {
        resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
      }
    });
  });
}

getJokePromise().then(function(result) {
  console.log(result);
  return getJokePromise();
}).then(function(result) {
  console.log(result);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this case, we end up sequentially executing two asynchronous requests. When the first Promise is resolved, the first joke is printed and a new Promise object is returned by the &lt;code&gt;then&lt;/code&gt; method. This new Promise object then has &lt;code&gt;then&lt;/code&gt; called on it. When the Promise succeeds, the &lt;code&gt;then&lt;/code&gt; success callback is triggered and the second joke is printed.&lt;/p&gt;

&lt;p&gt;The takeaway from all this is that calling &lt;code&gt;return&lt;/code&gt; in a &lt;code&gt;then&lt;/code&gt; callback will always result in returning a Promise object. It is this that allows for &lt;code&gt;then&lt;/code&gt; chaining!&lt;/p&gt;

&lt;h3 id=&#34;error-handling:e45390df332d7d109b53a5ac1df46eec&#34;&gt;Error handling&lt;/h3&gt;

&lt;p&gt;We mentioned in the previous section how the &lt;code&gt;then&lt;/code&gt; method can take an optional failure callback that gets triggered when &lt;code&gt;reject&lt;/code&gt; is called. It is customary to reject with an Error object as they capture a stack trace, thereby facilitating debugging.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &amp;quot;http://random.url.com&amp;quot;,
    success: function(result) {
      resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
    },
    error: function(jqxhr, textStatus) {
      reject(Error(&amp;quot;The AJAX request failed.&amp;quot;));
    }
  });
});

promise.then(function(result) {
  console.log(result);
}, function(error) {
  console.log(error);
  console.log(error.stack);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Personally, I find this a bit hard to read. Luckily we can use the &lt;code&gt;catch&lt;/code&gt; method to make this look a bit nicer. There&amp;rsquo;s nothing special about the &lt;code&gt;catch&lt;/code&gt; method. In fact, it&amp;rsquo;s just sugar for &lt;code&gt;then(undefined, func)&lt;/code&gt;, but it definitely makes code easier to read.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  $.ajax({
    url: &amp;quot;http://random.url.com&amp;quot;,
    success: function(result) {
      resolve(result[&amp;quot;value&amp;quot;][&amp;quot;joke&amp;quot;]);
    },
    error: function(jqxhr, textStatus) {
      reject(Error(&amp;quot;The AJAX request failed.&amp;quot;));
    }
  });
});

promise.then(function(result) {
  console.log(result);
}).then(function(result) {
  console.log(&amp;quot;foo&amp;quot;); // gets skipped
}).then(function(result) {
  console.log(&amp;quot;bar&amp;quot;); // gets skipped
}).catch(function(error) {
  console.log(error);
  console.log(error.stack);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Aside from illustrating improved readability, the above code showcases another aspect of the &lt;code&gt;reject&lt;/code&gt; method in that Promise rejections will cause your code to skip forward to the next &lt;code&gt;then&lt;/code&gt; method that has a rejection callback (or the next &lt;code&gt;catch&lt;/code&gt; method, since this is equivalent). It is this fallthrough behavior that causes this code to not print &amp;ldquo;foo&amp;rdquo; or &amp;ldquo;bar&amp;rdquo;!&lt;/p&gt;

&lt;p&gt;As a final point, it is useful to know that a Promise is implicitly rejected if an error is thrown in its constructor callback. This means it&amp;rsquo;s useful to do all your Promise related work inside the Promise constructor callback, so errors automatically become rejections.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var promise = new Promise(function(resolve, reject) {
  // JSON.parse throws an error if you feed it some
  // invalid JSON, so this implicitly rejects
  JSON.parse(&amp;quot;This ain&#39;t JSON&amp;quot;);
});

promise.then(function(result) {
  console.log(result);
}).catch(function(error) {
  console.log(error);
});
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code will cause the Promise to be rejected and an error to be printed because it will fail to parse the invalid JSON string.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unwanted spot instance termination in multi-AZ ASG</title>
      <link>https://vaneyckt.io/posts/unwanted_spot_instance_termination_in_multi_az_asg/</link>
      <pubDate>Sat, 24 Jan 2015 19:17:53 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/unwanted_spot_instance_termination_in_multi_az_asg/</guid>
      <description>&lt;p&gt;An auto scaling group is an AWS abstraction that facilitates increasing or decreasing the number of EC2 instances within your application&amp;rsquo;s architecture. Spot instances are unused AWS servers that are auctioned off for little money. The combination of these two allows for large auto scaling groups at low costs. However, you can lose your spot instances at a moment&amp;rsquo;s notice as soon as someone out there wants to pay more than you do.&lt;/p&gt;

&lt;p&gt;Knowing all this, I recently found myself looking into why AWS was terminating several of our spot instances every day. We were bidding 20% over the average price, so it seemed unlikely that this was being caused by a monetary issue. Nevertheless, we kept noticing multiple spot instances disappearing on a daily basis.&lt;/p&gt;

&lt;p&gt;It took a while to get to the bottom of things, but it turned out that this particular problem was being caused by an unfortunate combination of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;our auto scaling group spanning multiple availability zones&lt;/li&gt;
&lt;li&gt;our scaling code making calls to &lt;code&gt;TerminateInstanceInAutoScalingGroup&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The step-by-step explanation of this issue was as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;our scaling code was asking AWS to put 10 instances in our auto scaling group&lt;/li&gt;
&lt;li&gt;AWS obliged and put 5 instances in availability zone A and another 5 in zone B&lt;/li&gt;
&lt;li&gt;some time later our scaling code would decide that 2 specific instances were no longer needed. A call would be made to &lt;code&gt;TerminateInstanceInAutoScalingGroup&lt;/code&gt; to have just these 2 specific instances terminated.&lt;/li&gt;
&lt;li&gt;if these 2 instances happened to be in the same availability zone, then one zone would now have 3 instances, while the other one would now have 5&lt;/li&gt;
&lt;li&gt;AWS would detect that both zones were no longer balanced and would initiate a rebalancing action. This rebalancing action would terminate one of the instances in the zone with 5 instances, and spin up another instance in the zone with 3 instances.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So while this action did indeed end up rebalancing the instances across the different availability zones, it also inadvertently ended up terminating a running instance.&lt;/p&gt;

&lt;p&gt;The relevant entry from the &lt;a href=&#34;http://awsdocs.s3.amazonaws.com/AutoScaling/latest/as-dg.pdf&#34;&gt;AWS Auto Scaling docs&lt;/a&gt; is shown below.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Instance Distribution and Balance across Multiple Zones&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group. Auto Scaling attempts to launch new instances in the Availability Zone with the fewest instances. If the attempt fails, however, Auto Scaling will attempt to launch in other zones until it succeeds.&lt;/p&gt;

&lt;p&gt;Certain operations and conditions can cause your Auto Scaling group to become unbalanced. Auto Scaling compensates by creating a rebalancing activity under any of the following conditions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;You issue a request to change the Availability Zones for your group.&lt;/li&gt;
&lt;li&gt;You call &lt;code&gt;TerminateInstanceInAutoScalingGroup&lt;/code&gt;, which causes the group to become unbalanced.&lt;/li&gt;
&lt;li&gt;An Availability Zone that previously had insufficient capacity recovers and has additional capacity available.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Auto Scaling always launches new instances before attempting to terminate old ones, so a rebalancing activity will not compromise the performance or availability of your application.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Multi-Zone Instance Counts when Approaching Capacity&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Because Auto Scaling always attempts to launch new instances before terminating old ones, being at or near the specified maximum capacity could impede or completely halt rebalancing activities. To avoid this problem, the system can temporarily exceed the specified maximum capacity of a group by a 10 percent margin during a rebalancing activity (or by a 1-instance margin, whichever is greater). The margin is extended only if the group is at or near maximum capacity and needs rebalancing, either as a result of user-requested rezoning or to compensate for zone availability issues. The extension lasts only as long as needed to rebalance the group—typically a few minutes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I&amp;rsquo;m not sure about the best way to deal with this behavior. In our case, we just restricted our auto scaling group to one availability zone. This was good enough for us as none of the work done by our spot instances is critical. Going through the &lt;a href=&#34;http://awsdocs.s3.amazonaws.com/AutoScaling/latest/as-dg.pdf&#34;&gt;docs&lt;/a&gt;, it seems one approach might be to disable the &lt;code&gt;AZRebalance&lt;/code&gt; process. However, I have not had the chance to try this, so I cannot guarantee a lack of unexpected side effects.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating an EC2 Instance in a VPC with the AWS CLI</title>
      <link>https://vaneyckt.io/posts/creating_an_ec2_instance_in_a_vpc_with_the_aws_cli/</link>
      <pubDate>Wed, 29 Oct 2014 17:36:12 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/creating_an_ec2_instance_in_a_vpc_with_the_aws_cli/</guid>
      <description>

&lt;p&gt;Setting up an EC2 instance on AWS used to be as straightforward as provisioning a machine and SSHing into it. However, this process has become a bit more complicated now that Amazon VPC has become the standard for managing machines in the cloud.&lt;/p&gt;

&lt;p&gt;So what exactly is a Virtual Private Cloud? Amazon defines a VPC as &amp;lsquo;a logically isolated section of the AWS Cloud&amp;rsquo;. Instances inside a VPC can by default only communicate with other instances in the same VPC and are therefore invisible to the rest of the internet. This means they will not accept SSH connections coming from your computer, nor will they respond to any http requests. In this article we&amp;rsquo;ll look into changing these default settings into something more befitting a general purpose server.&lt;/p&gt;

&lt;h3 id=&#34;setting-up-your-vpc:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Setting up your VPC&lt;/h3&gt;

&lt;p&gt;Start by installing the &lt;a href=&#34;http://aws.amazon.com/cli&#34;&gt;AWS Command Line Interface&lt;/a&gt; on your machine if you haven&amp;rsquo;t done so already. With this done, we can now create our VPC.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ vpcId=`aws ec2 create-vpc --cidr-block 10.0.0.0/28 --query &#39;Vpc.VpcId&#39; --output text`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are several interesting things here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;--cidr-block&lt;/code&gt; parameter specifies a /28 netmask that allows for 16 IP addresses. This is the smallest supported netmask.&lt;/li&gt;
&lt;li&gt;the &lt;code&gt;create-vpc&lt;/code&gt; command returns a JSON string. We can filter out specific fields from this string by using the &lt;code&gt;--query&lt;/code&gt; and &lt;code&gt;--output&lt;/code&gt; parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The next step is to overwrite the default VPC DNS settings. As mentioned earlier, instances launched inside a VPC are invisible to the rest of the internet by default. AWS therefore does not bother assigning them a public DNS name. Luckily this can be changed easily.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ aws ec2 modify-vpc-attribute --vpc-id $vpcId --enable-dns-support &amp;quot;{\&amp;quot;Value\&amp;quot;:true}&amp;quot;
$ aws ec2 modify-vpc-attribute --vpc-id $vpcId --enable-dns-hostnames &amp;quot;{\&amp;quot;Value\&amp;quot;:true}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adding-an-internet-gateway:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Adding an Internet Gateway&lt;/h3&gt;

&lt;p&gt;Next we need to connect our VPC to the rest of the internet by attaching an internet gateway. Our VPC would be isolated from the internet without this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ internetGatewayId=`aws ec2 create-internet-gateway --query &#39;InternetGateway.InternetGatewayId&#39; --output text`
$ aws ec2 attach-internet-gateway --internet-gateway-id $internetGatewayId --vpc-id $vpcId
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creating-a-subnet:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Creating a Subnet&lt;/h3&gt;

&lt;p&gt;A VPC can have multiple subnets. Since our use case only requires one, we can reuse the cidr-block specified during VPC creation so as to get a single subnet that spans the entire VPC address space.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ subnetId=`aws ec2 create-subnet --vpc-id $vpcId --cidr-block 10.0.0.0/28 --query &#39;Subnet.SubnetId&#39; --output text`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;While this &lt;code&gt;--cidr-block&lt;/code&gt; parameter specifies a subnet that can contain 16 IP addresses (10.0.0.1 - 10.0.0.16), AWS will reserve 5 of those for private use. While this doesn&amp;rsquo;t really have an impact on our use case, it is still good to be aware of such things.&lt;/p&gt;

&lt;h3 id=&#34;configuring-the-route-table:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Configuring the Route Table&lt;/h3&gt;

&lt;p&gt;Each subnet needs to have a route table associated with it to specify the routing of its outbound traffic. By default every subnet inherits the default VPC route table which allows for intra-VPC communication only.&lt;/p&gt;

&lt;p&gt;Here we add a route table to our subnet so as to allow traffic not meant for an instance inside the VPC to be routed to the internet through the internet gateway we created earlier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ routeTableId=`aws ec2 create-route-table --vpc-id $vpcId --query &#39;RouteTable.RouteTableId&#39; --output text`
$ aws ec2 associate-route-table --route-table-id $routeTableId --subnet-id $subnetId
$ aws ec2 create-route --route-table-id $routeTableId --destination-cidr-block 0.0.0.0/0 --gateway-id $internetGatewayId
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;adding-a-security-group:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Adding a Security Group&lt;/h3&gt;

&lt;p&gt;Before we can launch an instance, we first need to create a security group that specifies which ports should allow traffic. For now we&amp;rsquo;ll just allow anyone to try and make an SSH connection by opening port 22 to any IP address.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ securityGroupId=`aws ec2 create-security-group --group-name my-security-group --description &amp;quot;my-security-group&amp;quot; --vpc-id $vpcId --query &#39;GroupId&#39; --output text`
$ aws ec2 authorize-security-group-ingress --group-id $securityGroupId --protocol tcp --port 22 --cidr 0.0.0.0/0
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;launching-your-instance:ed16f33dbf5018a09918ad8ef3c5f741&#34;&gt;Launching your Instance&lt;/h3&gt;

&lt;p&gt;All that&amp;rsquo;s left to do is to create an SSH key pair and then launch an instance secured by this. Let&amp;rsquo;s generate this key pair and store it locally with the correct permissions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ aws ec2 create-key-pair --key-name my-key --query &#39;KeyMaterial&#39; --output text &amp;gt; ~/.ssh/my-key.pem
$ chmod 400 ~/.ssh/my-key.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now launch a single t2.micro instance based on the public AWS Ubuntu image.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ instanceId=`aws ec2 run-instances --image-id ami-9eaa1cf6 --count 1 --instance-type t2.micro --key-name my-key --security-group-ids $securityGroupId --subnet-id $subnetId --associate-public-ip-address --query &#39;Instances[0].InstanceId&#39; --output text`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a few minutes your instance should be up and running. You should now be able to obtain the url of your active instance and SSH into it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ instanceUrl=`aws ec2 describe-instances --instance-ids $instanceId --query &#39;Reservations[0].Instances[0].PublicDnsName&#39; --output text`
$ ssh -i ~/.ssh/my-key.pem ubuntu@$instanceUrl
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. It&amp;rsquo;s really not all that hard. There&amp;rsquo;s just an awful lot of concepts that you need to get your head around which can make it a bit daunting at first. Be sure to check out the free &lt;a href=&#34;http://www.amazon.com/gp/product/B007S33NT2/ref=cm_cr_ryp_prd_ttl_sol_0&#34;&gt;Amazon Virtual Private Cloud User Guide&lt;/a&gt; if you want to learn more about VPCs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Finding and deleting old tags in a Github repository</title>
      <link>https://vaneyckt.io/posts/finding_and_deleting_old_tags_in_a_github_repo/</link>
      <pubDate>Fri, 18 Jul 2014 19:32:12 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/finding_and_deleting_old_tags_in_a_github_repo/</guid>
      <description>

&lt;p&gt;It&amp;rsquo;s very easy for a Github repository to accumulate lots of tags over time. This onslaught of tags tends to be tolerated until it starts impacting git performance. It is at this point, when you have well in excess of tens of thousands of tags, that a call to action tends to be made. In this article, we&amp;rsquo;ll look at two approaches to rid yourself of these old tags.&lt;/p&gt;

&lt;h3 id=&#34;the-cut-off-tag-approach:257dca9811faca70d0b7c4cba16ee1c1&#34;&gt;The cut-off tag approach&lt;/h3&gt;

&lt;p&gt;This approach has us specify a cut-off tag. All tags that can trace their ancestry back to this cut-off tag will be allowed to remain. All others will get deleted. This is especially useful for when you have just merged a new feature, and now you want to delete all tags that were created before this merge. In this scenario, all you have to do is tag the merge commit and then use this as the cut-off tag.&lt;/p&gt;

&lt;p&gt;The sequence of commands below deletes all tags that do not have the release-5 tag as an ancestor. Most of these commands are pretty self-explanatory, except for the one in the middle. The remainder of this section will focus on explaining this command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# fetch all tags from the remote
git fetch

# delete all tags on the remote that do not have the release-5 tag as an ancestor
comm -23 &amp;lt;(git tag | sort) &amp;lt;(git tag --contains release-5 | sort) | xargs git push --delete origin

# delete all local tags that are no longer present on the remote
git fetch --prune origin +refs/tags/*:refs/tags/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;a href=&#34;http://linux.die.net/man/1/comm&#34;&gt;comm command&lt;/a&gt; is used to &lt;a href=&#34;http://www.unixcl.com/2009/08/linux-comm-command-brief-tutorial.html&#34;&gt;compare two sorted files line by line&lt;/a&gt;. Luckily, we can avoid having to create any actual files by relying on process substitution instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;comm -23 &amp;lt;(command to act as file 1) &amp;lt;(command to act as file 2) | xargs git push --delete origin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-23&lt;/code&gt; flag tells &lt;code&gt;comm&lt;/code&gt; to suppress any lines that are unique to file 2, as well as any lines that appear in both files. In other words, it causes &lt;code&gt;comm&lt;/code&gt; to return just those lines that only appear in file 1. Looking back at our sequence of commands above, it should be clear that this will cause us to obtain all tags that do not have the release-5 tag as an ancestor. Piping this output to &lt;code&gt;xargs git push --delete origin&lt;/code&gt; will then remove these tags from Github.&lt;/p&gt;

&lt;h3 id=&#34;the-cut-off-date-approach:257dca9811faca70d0b7c4cba16ee1c1&#34;&gt;The cut-off date approach&lt;/h3&gt;

&lt;p&gt;While the cut-off tag approach works great in a lot of scenarios, sometimes you just want to delete all tags that were created before a given cut-off date instead. Unfortunately, git doesn&amp;rsquo;t have any built-in functionality for accomplishing this. This is why we are going to make use of a Ruby script here.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# CUT_OFF_DATE needs to be of YYYY-MM-DD format
CUT_OFF_DATE = &amp;quot;2015-05-10&amp;quot;

def get_old_tags(cut_off_date)  
  `git log --tags --simplify-by-decoration --pretty=&amp;quot;format:%ai %d&amp;quot;`
  .split(&amp;quot;\n&amp;quot;)
  .each_with_object([]) do |line, old_tags|
    if line.include?(&amp;quot;tag: &amp;quot;)
      date = line[0..9]
      tags = line[28..-2].gsub(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;).concat(&amp;quot; &amp;quot;).scan(/tag: (.*?) /).flatten
      old_tags.concat(tags) if date &amp;lt; cut_off_date
    end
  end
end

# fetch all tags from the remote
`git fetch`

# delete all tags on the remote that were created before the CUT_OFF_DATE
get_old_tags(CUT_OFF_DATE).each_slice(100) do |batch|
  system(&amp;quot;git&amp;quot;, &amp;quot;push&amp;quot;, &amp;quot;--delete&amp;quot;, &amp;quot;origin&amp;quot;, *batch)
end

# delete all local tags that are no longer present on the remote
`git fetch --prune origin +refs/tags/*:refs/tags/*`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This Ruby script should be pretty straightforward. The &lt;code&gt;get_old_tags&lt;/code&gt; method might stand out a bit here. It can look pretty complex, but most of it is just string manipulation to get the date and tags of each line outputted by the &lt;code&gt;git log&lt;/code&gt; command, and storing old tags in the &lt;code&gt;old_tags&lt;/code&gt; array. Note how we invoke the &lt;code&gt;system&lt;/code&gt; method with an array of arguments for those calls that require input. This protects us against possible shell injection.&lt;/p&gt;

&lt;p&gt;Be careful, as running this exact script inside your repository will delete all tags created before 2015-05-10. Also, be sure to specify your cut-off date in YYYY-MM-DD format!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adding a post-execution hook to the db:migrate task</title>
      <link>https://vaneyckt.io/posts/adding_a_post_execution_hook_to_the_db_migrate_task/</link>
      <pubDate>Mon, 09 Jun 2014 16:31:22 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/adding_a_post_execution_hook_to_the_db_migrate_task/</guid>
      <description>&lt;p&gt;A few days ago we discovered that our MySQL database&amp;rsquo;s default character set and collation had been changed to the wrong values. Worse yet, it looked like this change had happened many months ago; something which we had been completely unaware of until now! In order to make sure this didn&amp;rsquo;t happen again, we looked into adding a post-execution hook to the rails db:migrate task.&lt;/p&gt;

&lt;p&gt;Our first attempt is shown below. Here, we append a post-execution hook to the existing db:migrate task by creating a new db:migrate task. In rake, when a task is defined twice, the behavior of the new task gets appended to the behavior of the old task. So even though the code below may give the impression of overwriting the rails db:migrate task, we are actually just appending a call to the &lt;code&gt;post_execution_hook&lt;/code&gt; method to it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;namespace :db do
  def post_execution_hook
    puts &#39;This code gets run after the rails db:migrate task.&#39;
    puts &#39;However, it only runs if the db:migrate task does not throw an exception.&#39;
  end

  task :migrate do
    post_execution_hook
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the above example only runs the appended code if the original db:migrate task does not throw any exceptions. Luckily we can do better than that by taking a slightly different approach. Rather than appending code, we are going to have a go at prepending it instead.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;namespace :db do
  def post_execution_hook
    puts &#39;This code gets run after the rails db:migrate task.&#39;
    puts &#39;It will ALWAYS run.&#39;
  end

  task :attach_hook do
    at_exit { post_execution_hook }
  end
end

Rake::Task[&#39;db:migrate&#39;].enhance([&#39;db:attach_hook&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we make use of the &lt;a href=&#34;http://ruby-doc.org/stdlib-2.0.0/libdoc/rake/rdoc/Rake/Task.html#method-i-enhance&#34;&gt;enhance method&lt;/a&gt; to add db:attach_hook as a prerequisite task to db:migrate. This means that calling db:migrate will now cause the db:attach_hook task to get executed before db:migrate gets run. The db:attach_hook task creates an &lt;code&gt;at_exit&lt;/code&gt; hook that will trigger our post-execution code upon exit of the db:migrate task. Hence, our post-execution hook will now get called even when db:migrate raises an exception!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Installing chromedriver</title>
      <link>https://vaneyckt.io/posts/installing_chromedriver/</link>
      <pubDate>Wed, 14 May 2014 20:14:48 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/installing_chromedriver/</guid>
      <description>&lt;p&gt;Some time ago I needed to install &lt;a href=&#34;https://sites.google.com/a/chromium.org/chromedriver/&#34;&gt;chromedriver&lt;/a&gt; on a ubuntu machine. While this wasn&amp;rsquo;t too hard, I was nevertheless surprised by the number of open StackOverflow questions on this topic. So I decided to leave some notes for my future self.&lt;/p&gt;

&lt;p&gt;First of all, let&amp;rsquo;s install chromedriver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ LATEST_RELEASE=$(curl http://chromedriver.storage.googleapis.com/LATEST_RELEASE)
$ wget http://chromedriver.storage.googleapis.com/$LATEST_RELEASE/chromedriver_linux64.zip
$ unzip chromedriver_linux64.zip
$ rm chromedriver_linux64.zip
$ sudo mv chromedriver /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s see what happens when we try and run it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chromedriver

chromedriver: error while loading shared libraries: libgconf-2.so.4:
cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s a bit unexpected. Luckily we can easily fix this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo apt-get install libgconf-2-4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have a functioning chromedriver, the only thing left to do is to install Chrome. After all, chromedriver can&amp;rsquo;t function without Chrome.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
$ sudo sh -c &#39;echo &amp;quot;deb http://dl.google.com/linux/chrome/deb/ stable main&amp;quot; &amp;gt;&amp;gt; /etc/apt/sources.list.d/google.list&#39;
$ sudo apt-get update
$ sudo apt-get install google-chrome-stable
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. You should be good to go now.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programmatically rotating the Android screen</title>
      <link>https://vaneyckt.io/posts/programmatically_rotating_the_android_screen/</link>
      <pubDate>Thu, 20 Mar 2014 20:08:17 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/programmatically_rotating_the_android_screen/</guid>
      <description>&lt;p&gt;A lot of digital ink has been spilled on this subject, so I figured it might be worth to briefly talk about this. You can either change the orientation through ADB or through an app. While the ADB approach is the easiest, it might not work on all devices or on all Android versions. For example, the &lt;code&gt;dumpsys&lt;/code&gt; output of a Kindle Fire is different than that of a Samsung Galaxy S4, so you might need to tweak the grepping of the output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get current orientation
adb shell dumpsys input | grep SurfaceOrientation | awk &#39;{print $2}&#39;

# change orientaton to portait
adb shell content insert --uri content://settings/system --bind name:s:accelerometer_rotation --bind value:i:0
adb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:0

# change orientation to landscape
adb shell content insert --uri content://settings/system --bind name:s:accelerometer_rotation --bind value:i:0
adb shell content insert --uri content://settings/system --bind name:s:user_rotation --bind value:i:1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don’t want to use ADB and prefer to change the orientation through an Android app instead, then you can just use these commands.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;// get current orientation
final int orientation = myActivity.getResources().getConfiguration().orientation;

// change orientation to portrait
myActivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_PORTRAIT);

// change orientation to landscape
myActivity.setRequestedOrientation(ActivityInfo.SCREEN_ORIENTATION_LANDSCAPE);
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Programmatically creating Android touch events</title>
      <link>https://vaneyckt.io/posts/programmatically_creating_android_touch_events/</link>
      <pubDate>Tue, 04 Mar 2014 20:40:56 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/programmatically_creating_android_touch_events/</guid>
      <description>&lt;p&gt;Recent versions of Android have the &lt;code&gt;adb shell input touch&lt;/code&gt; functionality to simulate touch events on an Android device or simulator. However, older Android versions (like 2.3) do not support this command. Luckily it is possible to recreate this functionality by running &lt;code&gt;adb shell getevent&lt;/code&gt; to capture events as they are being generated. These events can then later be replayed using the &lt;code&gt;adb shell sendevent&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Running &lt;code&gt;adb shell getevent&lt;/code&gt; when touching the screen might get you something like shown below. Notice how the output is in hexadecimal.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/dev/input/event7: 0001 014a 00000001
/dev/input/event7: 0003 003a 00000001
/dev/input/event7: 0003 0035 000001ce
/dev/input/event7: 0003 0036 00000382
/dev/input/event7: 0000 0002 00000000
/dev/input/event7: 0000 0000 00000000
/dev/input/event7: 0001 014a 00000000
/dev/input/event7: 0003 003a 00000000
/dev/input/event7: 0003 0035 000001ce
/dev/input/event7: 0003 0036 00000382
/dev/input/event7: 0000 0002 00000000
/dev/input/event7: 0000 0000 00000000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the &lt;code&gt;adb shell sendevent&lt;/code&gt; command expect all of its input to be in decimal. So if we wanted to replay the above events, we&amp;rsquo;d need to do something like shown below. Note that 462 and 898 are the x and y coordinates of this particular touch event.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;adb shell sendevent /dev/input/event7: 1 330 1
adb shell sendevent /dev/input/event7: 3 58 1
adb shell sendevent /dev/input/event7: 3 53 462
adb shell sendevent /dev/input/event7: 3 54 898
adb shell sendevent /dev/input/event7: 0 2 0
adb shell sendevent /dev/input/event7: 0 0 0
adb shell sendevent /dev/input/event7: 1 330 0
adb shell sendevent /dev/input/event7: 3 58 0
adb shell sendevent /dev/input/event7: 3 53 462
adb shell sendevent /dev/input/event7: 3 54 898
adb shell sendevent /dev/input/event7: 0 2 0
adb shell sendevent /dev/input/event7: 0 0 0
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Some lesser known Github API functionality</title>
      <link>https://vaneyckt.io/posts/some_lesser_known_github_api_functionality/</link>
      <pubDate>Sat, 08 Feb 2014 18:05:12 +0000</pubDate>
      
      <guid>https://vaneyckt.io/posts/some_lesser_known_github_api_functionality/</guid>
      <description>

&lt;p&gt;One of our automation tools occasionally needs to interact with our Github repositories. Unfortunately, the current implementation of this tool leaves something to be desired as it requires cloning these repositories to local disk. Changes against these local repositories are then made on local branches, after which these branches get pushed to Github.&lt;/p&gt;

&lt;p&gt;However, in order to save on disk space this tool will only ever create a single local copy of each repository. This makes it unsafe to run multiple instances of this tool as multiple instances simultaneously executing sequences of git commands against the same local repositories might lead to these commands inadvertently getting interpolated, thereby leaving the local repositories in an undefined state.&lt;/p&gt;

&lt;p&gt;The solution to this complexity was to completely remove the need for local repositories and instead aim to have everything done through the wonderful Github API. This article is a reminder to myself about some API functionality that I found while looking into this.&lt;/p&gt;

&lt;h3 id=&#34;checking-if-a-branch-contains-a-commit:ebeddf967e2b970d374ec29d22d794b8&#34;&gt;Checking if a branch contains a commit&lt;/h3&gt;

&lt;p&gt;While the Github API does not have an explicit call to check whether a given commit is included in a branch, we can nevertheless use the &lt;a href=&#34;https://developer.github.com/v3/repos/commits/#compare-two-commits&#34;&gt;compare call&lt;/a&gt; for just this purpose. This call takes two commits as input and returns a large JSON response of comparison data. We can use the &lt;code&gt;status&lt;/code&gt; field of the response to ascertain if a given commit is behind or identical to the HEAD commit of a branch. If so, then the branch contains that commit.&lt;/p&gt;

&lt;p&gt;We can use the &lt;a href=&#34;https://github.com/octokit/octokit.rb&#34;&gt;Ruby octokit gem&lt;/a&gt; to implement this as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;octokit&#39;

class GithubClient &amp;lt; Octokit::Client
  def branch_contains_sha?(repo, branch, sha)
    [&#39;behind&#39;, &#39;identical&#39;].include?(compare(repo, branch, sha).status)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creating-a-remote-branch-from-a-remote-commit:ebeddf967e2b970d374ec29d22d794b8&#34;&gt;Creating a remote branch from a remote commit&lt;/h3&gt;

&lt;p&gt;Sometimes you&amp;rsquo;ll want to create a remote branch by branching from a remote commit. We can use the &lt;a href=&#34;https://developer.github.com/v3/git/refs/#create-a-reference&#34;&gt;create_reference call&lt;/a&gt; to accomplish this. Note that the &lt;code&gt;ref&lt;/code&gt; parameter of this call needs to be set to &lt;code&gt;refs/heads/#{branch}&lt;/code&gt; when creating a remote branch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;octokit&#39;

class GithubClient &amp;lt; Octokit::Client
  def create_branch_from_sha(repo, branch, sha)
    # create_ref internally transforms &amp;quot;heads/#{branch}&amp;quot; into &amp;quot;refs/heads/#{branch}&amp;quot;
    # as mentioned above, this is required by the Github API
    create_ref(repo, &amp;quot;heads/#{branch}&amp;quot;, sha)
  end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setting-the-head-of-a-remote-branch-to-a-specific-remote-commit:ebeddf967e2b970d374ec29d22d794b8&#34;&gt;Setting the HEAD of a remote branch to a specific remote commit&lt;/h3&gt;

&lt;p&gt;You can even forcefully set the HEAD of a remote branch to a specific remote commit by using the &lt;a href=&#34;https://developer.github.com/v3/git/refs/#update-a-reference&#34;&gt;update_reference call&lt;/a&gt;. As mentioned earlier, the &lt;code&gt;ref&lt;/code&gt; parameter needs to be set to &lt;code&gt;refs/heads/#{branch}&lt;/code&gt;. Be careful when using this functionality though as it essentially allows you to overwrite the history of a remote branch!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;octokit&#39;

class GithubClient &amp;lt; Octokit::Client
  def update_branch_to_sha(repo, branch, sha, force = true)
    # update_ref internally transforms &amp;quot;heads/#{branch}&amp;quot; into &amp;quot;refs/heads/#{branch}&amp;quot;
    # as mentioned earlier, this is required by the Github API
    update_ref(repo, &amp;quot;heads/#{branch}&amp;quot;, sha, force)
  end
end
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>